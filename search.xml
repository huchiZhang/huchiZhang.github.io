<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>搜索算法2</title>
    <url>/2020/06/13/searchAlg2/</url>
    <content><![CDATA[<h2 id="状态和状态空间"><a href="#状态和状态空间" class="headerlink" title="状态和状态空间"></a>状态和状态空间</h2><h3 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h3><h5 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h5><ul>
<li>表示系统状态、事实等叙述性知识的一组变量或数组。</li>
</ul>
<h5 id="操作算子"><a href="#操作算子" class="headerlink" title="操作算子"></a>操作算子</h5><ul>
<li>描述状态之间的关系</li>
</ul>
<h5 id="状态空间"><a href="#状态空间" class="headerlink" title="状态空间"></a>状态空间</h5><ul>
<li>利用状态变量和操作符号，表示系统或问题的符号体系，状态空间是一个四元组：（S,O,So,G）<ul>
<li>S:状态集合</li>
<li>O:操作算子集合</li>
<li>So：包含问题的初始状态</li>
<li>G：目标状态集合</li>
</ul>
</li>
</ul>
<h3 id="2-状态空间的解"><a href="#2-状态空间的解" class="headerlink" title="2. 状态空间的解"></a>2. 状态空间的解</h3><h5 id="求解的路径"><a href="#求解的路径" class="headerlink" title="求解的路径"></a>求解的路径</h5><ul>
<li>从So节点到G节点的路径</li>
</ul>
<h5 id="状态空间的一个解"><a href="#状态空间的一个解" class="headerlink" title="状态空间的一个解"></a>状态空间的一个解</h5><p><img src="/2020/06/13/searchAlg2/image-20200613110101654.png" alt="image-20200613110101654"></p>
<h5 id="求解思路"><a href="#求解思路" class="headerlink" title="求解思路"></a>求解思路</h5><ol>
<li>设定状态变量及确定值域</li>
<li>确定状态组，分别列出初始状态集和目标状态集</li>
<li>定义并确定操作算子集</li>
<li>估计全部状态空间数，并尽可能列出全部状态空间或进行描述</li>
<li>当状态空间数量不是很大时，按问题的有序元祖画出状态空间图，按照图搜索算法对其进行求解</li>
</ol>
<h5 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h5><p><strong>传教士与野人问题</strong></p>
<p>在河的左岸有三个传教士、一条船和三个野人，传教士们想用这条船将所有的成员都运过河去，但是受到以下条件的限制：</p>
<ul>
<li>传教士和野人都会划船，但是船一次只能装两个</li>
<li>在任何一个岸边野人数不能超过传教士，苟泽传教士就会有危险</li>
</ul>
<p>解题步骤：</p>
<ul>
<li><p>设定状态变量及确定值域</p>
<ul>
<li><strong>设左岸传教士数为</strong>m，则m ={0,1,2,3}；对应右岸的传教士数为3－m；</li>
<li>设左岸传教士数为c，则c ={0,1,2,3}；对应右岸的传教士数为3－c；</li>
<li>设左岸船数为b，则b={0,1};对应右岸的船数为3－b；</li>
</ul>
</li>
<li><p>确定状态组，分别列出初始状态集和目标状态集</p>
<ul>
<li>建立该问题的状态空间，使用一个三元组 SK  = (m,c,b)</li>
<li>初始状态：(3,3,1)</li>
<li>目标状态：(0,0,0)</li>
</ul>
</li>
<li><p>定义并确定操作算子集</p>
<ul>
<li>以河的左岸为基点来考虑，把船<strong>从左岸划向右岸</strong>定义为<strong>Pij</strong>操作。其中,第一下标i表示船载的传教士数, 第二下标j表示船载的野人数；同理，<strong>从右岸将船划回左岸</strong>称之为<strong>Qij</strong>操作，下标的定义同前。则共有10种操作，操作集为</li>
<li><strong>F={P01，P10，P11，P02，P20，Q01，Q10，Q11，Q02，Q20}</strong></li>
</ul>
</li>
<li><p>估计全部状态空间数，并尽可能列出全部状态空间或进行描述</p>
<ul>
<li><p>在这个问题世界中，S0 =（3,3,1）为初始状态，S31 = Sg =（0,0,0）为目标状态。全部的可能状态共有32个，如表所示。</p>
</li>
<li><p><img src="/2020/06/13/searchAlg2/clip_image012_2.jpg" alt="img"></p>
<blockquote>
<p><strong>注意：</strong>按题目规定条件，应划去非法状态，从而加快搜索效率。</p>
<ol>
<li><p><strong>首先可以划去左岸边</strong>野人数目超过传教士的情况，即S4、S8、S9、S20、S24、S25等6种状态是不合法的；</p>
</li>
<li><p><strong>应划去右岸边</strong>野人<strong>数目超过修道士的情况，即S6、S7、S11、S22、S23、S27等情况；</strong></p>
</li>
<li><p>应划去4种不可能出现状态：划去S15和S16——船不可能停靠在无人的岸边；划去S3——传教士不可能在数量占优势的<strong>野人</strong>眼皮底下把船安全地划回来；划去S28——传教士也不可能在数量占优势的<strong>野人</strong>眼皮底下把船安全地划向对岸。可见，在状态空间中，真正符合题目规定条件的<strong>只有16个合理状态</strong>。</p>
</li>
</ol>
</blockquote>
</li>
</ul>
</li>
<li><p>当状态空间数量不是很大时，按问题的有序元祖画出状态空间图，按照图搜索算法对其进行求解</p>
<ul>
<li><p>画出的状态空间图如下图，图中任意一条从S0到达S31的路径都是该问题的解。</p>
<p><img src="/2020/06/13/searchAlg2/image-20200613121549557.png" alt="image-20200613121549557"></p>
</li>
<li><p>状态空间的建立，以BFS的方式为例，从初始节点开始根据操作集建立树，遇到不可能的节点就停止继续建造其所在子树</p>
<p><img src="/2020/06/13/searchAlg2/image-20200613120600674.png" alt="image-20200613120600674"></p>
</li>
<li><p>找到其中从初始状态到目标状态的路径</p>
<p><img src="/2020/06/13/searchAlg2/image-20200613120750089.png" alt="image-20200613120750089"></p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p> 参考：</p>
<ol>
<li><a href="https://www.cnblogs.com/6DAN_HUST/archive/2010/08/23/1806560.html" target="_blank" rel="noopener">人工智能——状态空间搜索及状态空间表示法</a></li>
</ol>
</blockquote>
<h2 id="盲目搜索（无信息搜索）"><a href="#盲目搜索（无信息搜索）" class="headerlink" title="盲目搜索（无信息搜索）"></a>盲目搜索（无信息搜索）</h2><p>盲目搜索是指在没有关于结论的条件下就按部就班的一个个搜索。盲目搜索策略是以节点扩展的次序来分类的（宽度优先，一致代价，深度优先，深度受限，迭代加深，双向搜索）。</p>
<blockquote>
<p>一般而言，指数级别复杂度的搜索问题不能用无信息的搜索算法求解，除非是规模很小的实例</p>
</blockquote>
<h4 id="1-广度优先搜索（BFS）"><a href="#1-广度优先搜索（BFS）" class="headerlink" title="1. 广度优先搜索（BFS）"></a>1. 广度优先搜索（BFS）</h4><p>最简单的盲目搜索过程就是广度优先搜索（breadth-first search）。该过程把所有的算子应用到开始节点以产生一个显示的状态空间图，再把所有可能的算子应用到开始节点的所有直接后继，再到后继的后继，等等。搜索过程一律从开始节点向外扩展。把所有可能的算子称为后继函数。当把后继函数应用到一个节点时，产生一个节点集。一个后继函数的每一次应用称为节点的扩展。</p>
<blockquote>
<p> <strong>bfs的性质</strong></p>
<ul>
<li>当问题有解时一定能找到解</li>
<li>当问题为单位耗散值，且问题有解时，一定能找到最优解、</li>
<li>方法与问题无关，具有通用性</li>
<li>效率较低</li>
</ul>
</blockquote>
<h5 id="算法流程："><a href="#算法流程：" class="headerlink" title="算法流程："></a>算法流程：</h5><ol>
<li>把起始节点放到OPEN表中(如果该起始节点为一目标节点，则求得一个解答)。</li>
<li>如果OPEN是个空表，则没有解，失败退出；否则继续。</li>
<li>把第一个节点(节点n)从OPEN表移出，并把它放入CLOSED的扩展节点表中。 扩展节点n。如果没有后继节点，则转向上述第(2)步。</li>
<li>把n的所有后继节点放到OPEN表末端，并提供从这些后继节点回到n的指针。</li>
<li>如果n的任一个后继节点是个目标节点，则找到一个解答，成功退出；否则转向第(2)步。</li>
</ol>
<p><img src="/2020/06/13/searchAlg2/20180704091506488" alt="img"></p>
<h4 id="2-深度优先搜索（DFS）"><a href="#2-深度优先搜索（DFS）" class="headerlink" title="2. 深度优先搜索（DFS）"></a>2. 深度优先搜索（DFS）</h4><p>深度优先总是扩展搜索树的当前边缘节点集 中最深的节点（搜索直接推到最深层）。如果最深层节点扩展完了，就回溯到下一个还有未扩展节点的深度稍浅的节点。</p>
<blockquote>
<p>DFS的性质：</p>
<ul>
<li>DFS的搜索效率严重依赖于使用的是图搜索还是树搜索<ul>
<li>如果是图搜索（避免了重复状态和冗余路径），那么DFS在有限状态空间就是完备的。</li>
<li>如果是树搜索，则不完备，因为会出现死循环（DFS算法本身是没有explored set的）。</li>
</ul>
</li>
<li>不是最优的</li>
<li>时间复杂度受限于状态空间的规模，为O(bm)，m是任一节点的最大深度。</li>
<li>空间复杂度很好，为O(bm)。所以DFS在AI的很多领域成为工作主力。</li>
</ul>
</blockquote>
<h5 id="算法流程：-1"><a href="#算法流程：-1" class="headerlink" title="算法流程："></a>算法流程：</h5><ol>
<li>把起始节点放到OPEN表中(如果该起始节点为一目标节点，则求得一个解答)。</li>
<li>如果OPEN是个空表，则没有解，失败退出；否则继续。</li>
<li>把第一个节点(节点n)从OPEN表移出，并把它放入CLOSED的扩展节点表中。</li>
<li>考察节点n是否为目标节点，若是，则找到问题的解，用回溯法求解路径，退出</li>
<li>如果没有后继节点，则转向上述第(2)步。</li>
<li>扩展节点n，把n的所有后继节点放到OPEN表前端，并提供从这些后继节点回到n的指针。转向第(2)步。</li>
</ol>
<p><img src="/2020/06/13/searchAlg2/20180704103614765" alt="img"></p>
<h4 id="3-BFS和DFS的比较"><a href="#3-BFS和DFS的比较" class="headerlink" title="3. BFS和DFS的比较"></a>3. BFS和DFS的比较</h4><h5 id="遍历同一组数据"><a href="#遍历同一组数据" class="headerlink" title="遍历同一组数据"></a>遍历同一组数据</h5><p>遍历方式不同，但都可以遍历到，时间相同。</p>
<p><img src="/2020/06/13/searchAlg2/1.gif" alt></p>
<h5 id="同时搜索一个数据"><a href="#同时搜索一个数据" class="headerlink" title="同时搜索一个数据"></a>同时搜索一个数据</h5><p>通过下面这个图可以看到深度优先搜索虽然更快的找到一个解但是并不是最优的。</p>
<p><img src="/2020/06/13/searchAlg2/2.gif" alt></p>
<h5 id="通过八数码的问题比较两个算法"><a href="#通过八数码的问题比较两个算法" class="headerlink" title="通过八数码的问题比较两个算法"></a>通过八数码的问题比较两个算法</h5><p>八数码问题：在3×3的方格棋盘上，摆放着1到8这八个数码，有1个方格是空的，其初始状态如图1所示，要求对空格执行空格左移、空格右移、空格上移和空格下移这四个操作使得棋盘从初始状态到目标状态。</p>
<p><img src="/2020/06/13/searchAlg2/11155718-dbdf23882ee74c03a6093e960551428f.png" alt="img"></p>
<p>广度有点搜索过程：</p>
<p><img src="/2020/06/13/searchAlg2/image-20200613165626335.png" alt="image-20200613165626335"></p>
<p>深度优先搜索过程：</p>
<p><img src="/2020/06/13/searchAlg2/image-20200613165801130.png" alt="image-20200613165801130"></p>
<h4 id="4-其他算法"><a href="#4-其他算法" class="headerlink" title="4. 其他算法"></a>4. 其他算法</h4><h5 id="A-一致代价搜索-UCS"><a href="#A-一致代价搜索-UCS" class="headerlink" title="A . 一致代价搜索(UCS)"></a>A . 一致代价搜索(UCS)</h5><p>一致代价搜索扩展的是路径消耗g(n)（从初始状态到当前状态的路径耗散）最小的节点n。（可以通过将边缘节点组织成按g值排序的队列来实现）。如果所有的连接弧线具有相等的代价,那么等代价算法就简化为宽度优先搜索算法,在等代价搜索算法中,不是描述沿着等长度路径断层进行的扩展,而是描述沿着等代价路径断层进行的扩展。</p>
<p><strong>算法流程：</strong></p>
<ol>
<li>把起始节点S放到未扩展节点表OPEN中。如果此起始节点为一目标节点,则求得一个解,否则令g(S)=0。</li>
<li>如果OPEN是个空表,则没有解而失败退出。</li>
<li>从OPEN表中选择一个节点,使其g()为最小。如果有几个节点都合格,那么就要选择一个目标节点作为节点(要是有目标节点的话);否则,就从中选一个作为节点i。把节点i从OPEN表移至扩展节点表 CLOSED中。</li>
<li>如果节点i为目标节点,则求得一个解。</li>
<li>扩展节点i。如果没有后继节点,则转向第(2)步。</li>
<li>对于节点i的每个后继节点j,计算g(j)=g(i)+c(i,j),并把所有后继节点j放进OPEN表。提供回到节点i的指针。</li>
<li>转向第2步。</li>
</ol>
<p><strong>流程图：</strong></p>
<p><img src="/2020/06/13/searchAlg2/u=2390937715,1397457370&fm=173&app=25&f=JPEG" alt="img"></p>
<h5 id="B-深度受限"><a href="#B-深度受限" class="headerlink" title="B. 深度受限"></a>B. 深度受限</h5><p>设置界限l来避免DFS在无限状态空间下搜索失败的尴尬情况。即，深度为l的节点被当做最深层节点（没有后继节点）来对待。</p>
<h5 id="C-迭代加深的深度优先算法-iterative-deepening-search（IDS）"><a href="#C-迭代加深的深度优先算法-iterative-deepening-search（IDS）" class="headerlink" title="C. 迭代加深的深度优先算法 iterative deepening search（IDS）"></a>C. 迭代加深的深度优先算法 iterative deepening search（IDS）</h5><p>IDS = DFS + BFS。<br>可以说是结合了宽度优先和深度优先的优点了：</p>
<ul>
<li>空间复杂度：O(bd) （和DFS一样）</li>
<li>在分支因子有限时完备，在路径待机时节点深度的非递减函数时最优（和BFS一样）。</li>
</ul>
<p><img src="/2020/06/13/searchAlg2/20180704105848581" alt="这里写图片描述"></p>
<h5 id="D-双向搜索"><a href="#D-双向搜索" class="headerlink" title="D. 双向搜索"></a>D. 双向搜索</h5><p>一个从初始状态开始搜，一个从目标状态开始搜，当边缘有交集，就说明找到了解。</p>
<p>好处：如果两个都用BFS，那么复杂度就变成了O(bd/2)+O(bd/2)，这肯定是要远远小于O(bd)的。所以说减小了复杂度。</p>
<blockquote>
<p>参考：</p>
<p><a href="https://baijiahao.baidu.com/s?id=1621524116688929556&wfr=spider&for=pc" target="_blank" rel="noopener">人工智能盲目搜索三大法则，你要懂！</a></p>
<p><a href="[https://blog.csdn.net/weixin_39278265/article/details/80906740#%E4%B8%89%E6%97%A0%E4%BF%A1%E6%81%AF%E6%90%9C%E7%B4%A2%E7%9B%B2%E7%9B%AE%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5](https://blog.csdn.net/weixin_39278265/article/details/80906740#三无信息搜索盲目搜索策略)">人工智能第三章（1）——无信息搜索（盲目搜索）</a></p>
<p><a href="https://www.cnblogs.com/sillypudding/archive/2013/04/11/3014771.html" target="_blank" rel="noopener">人工智能实验4——用盲目搜索求解八数码问题</a></p>
<p><a href="https://www.cnblogs.com/Cccccz/p/11156000.html" target="_blank" rel="noopener">人工智能07 盲目搜索</a></p>
</blockquote>
]]></content>
      <categories>
        <category>搜索算法笔记</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>搜索算法</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow文件操作</title>
    <url>/2020/06/12/tensorflow-2/</url>
    <content><![CDATA[<h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><h4 id="2-1-文件读取流程"><a href="#2-1-文件读取流程" class="headerlink" title="2.1 文件读取流程"></a>2.1 文件读取流程</h4><ol>
<li><p>通用文件读取流程：</p>
<ul>
<li><p>文件读取共分为三个流程</p>
</li>
<li><p><img src="/2020/06/12/tensorflow-2/image-20200612110657327.png" alt="image-20200612110657327"></p>
</li>
<li><p>第一阶段：构建一个文件名队列，读取文件名</p>
<ul>
<li>tf.train.string_input_producer(string_tensor,shuffle=True)<ul>
<li>String_tensor：含有文件名+路径的一阶张量</li>
<li>num_epochs：过几遍数据，默认无限过数据，就是生成批处理队列如果过大文件不够就再次从头过几遍数据。</li>
<li>shuffle : 是否随机读数据</li>
<li>return 文件队列</li>
</ul>
</li>
</ul>
</li>
<li><p>第二阶段：使用读取器在文件名队列中选择文件读取数据，再进行解码</p>
<ul>
<li><strong>文件读取与解码</strong><ul>
<li>阅读器每次只读取一个样本</li>
<li>文本：<ul>
<li>读取：tf.TextLineReader</li>
<li>解码：tf.decode_csv()</li>
</ul>
</li>
<li>图片：<ul>
<li>读取：tf.WholeFileReader</li>
<li>解码：<ul>
<li>tf.image.decode_jepg(contents)</li>
<li>tf.image.decode_png(contents)<ul>
<li>将图片解码成uint8张量</li>
<li>return：张量类型，3-D形状[height,width,channels]</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>二进制文件：<ul>
<li>读取：tf.FixedLengthRecordReader(record_bytes)<ul>
<li>读取每个记录是固定数量字节的二进制文件</li>
</ul>
</li>
<li>解码：tf.decode_raw</li>
</ul>
</li>
<li>TFRecords文件:<ul>
<li>tf.TFRecordReader</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ol>
<li>上面所有的读取器都有一个共同的读取方法:read(file_quene),并且都会返回一个tensirs元祖</li>
<li>由于默认只会读取一个样本，所以如果想要进行批处理，需要使用tf.train.batch或tf.shuffle_batch进行批处理操作，便于之后指定每批次多个样本的训练。</li>
<li>key,value = 读取器.read(file_quene)<ul>
<li>key是文件名</li>
<li>value是一个样本</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<p>在解码阶段，默认所有的内容都解码成tf.uint8类型，如果之后需要转换成指定类型可使用tf.cast()</p>
</blockquote>
</li>
<li><p>第三阶段：批处理阶段，将上一步解码出来的数据放入批处理队列中</p>
<ul>
<li>tf.train,batch(tensors,batch_size,num_threads = 1,capacity = 32,name = None)<ul>
<li>读取指定个数的张量</li>
<li>tensor：可以使包含张量的列表，批处理的内容放到列表中</li>
<li>batch_size：从队列中读取批处理大小</li>
<li>num_threads：进入队列的线程数</li>
<li>capacity ： 整数，队列中元素的最大数</li>
<li>return：tensors</li>
</ul>
</li>
<li>tf.train.shuffle_batch</li>
</ul>
</li>
</ul>
</li>
<li><p>线程操作</p>
<p><img src="/2020/06/12/tensorflow-2/image-20200612121019674.png" alt="image-20200612121019674"></p>
</li>
</ol>
]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>学习笔记</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>问题目录</title>
    <url>/2020/06/11/202006/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="b49527322f67e2ec93481fe7ab1ea01e22c9c217977a397a85414acf783431dc">6da395bcc2661c4d53816fcc9cc69d03181c2fbc3c41d89115daf77d71a1b0b0c207263e326a8ee04cf85c222082cdf5130daf79bbd66f609ff860ffcf55b6110462b250283a13918bed303704fb6c906fe01e3a415c3e0c650c4bb9eae7b7ed673aebfeb69ddd5d94375a6ef89164d96f299ead3efc38c7ecc382c5432d11a7d94d0771467a3e62e1709fe00fb6e4475eacd0fb4b27ec4c298ec1014241de1d519ef8b54935753472df5da0f4eb678c3635f264e189a9bdb0ad22665de7b59564cb183c03236b860b12a1ed9fc07d16c3a85bd561984602a0657994fcdea542fbd06142cf9387c53e74bfcf575440a48f13e09a538e8a5bf9e37fecf41d38ea828d08a4f1392338907db0e467569ed0640f22ac97f396d13e3a88b53c64f79ecd9eaaf73e7a550a3c6cc6d91e55d8ea15a862bcecd6a691d89f4e94946a52eae5816f13cc3eff39f8ef5cadf49d2bcc33884757bf266d7ebf3d41ff88593ef0ecf73daf18bfed70ed3ea3204f38c05587ca6b77d9d82c10c03d834b1cf7e652bb6df52aef2bd169f4fa772d21b18d264b9d7414cb4c49d604a145d82f44bb33a0acd580e13f7e5ada22a710a0b175f2dcc4313569b4adb5995e424990643c4faa033f11fd0e5adfe564cf8e516c145ff881fb667551d43f57c38732f47576b87b9ebe83723f51bed4cab165153d0983</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>每月状态记录</category>
      </categories>
      <tags>
        <tag>状态</tag>
      </tags>
  </entry>
  <entry>
    <title>decisionTree1</title>
    <url>/2020/06/11/decisionTree1/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>人工智能笔记</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>搜索算法1</title>
    <url>/2020/06/10/searchAlg1/</url>
    <content><![CDATA[<h2 id="搜索问题说明"><a href="#搜索问题说明" class="headerlink" title="搜索问题说明"></a>搜索问题说明</h2><p>搜索问题是求解问题的一种方法。</p>
<h3 id="1-搜索问题要解决的几个基本问题"><a href="#1-搜索问题要解决的几个基本问题" class="headerlink" title="1. 搜索问题要解决的几个基本问题"></a>1. 搜索问题要解决的几个基本问题</h3><ul>
<li>是否一定能找到一个解</li>
<li>找到的解是否为最佳</li>
<li>时间和空间复杂度如何</li>
<li>是否终止运行或者陷入一个死循环</li>
</ul>
<h3 id="2-搜索的方向"><a href="#2-搜索的方向" class="headerlink" title="2. 搜索的方向"></a>2. 搜索的方向</h3><ul>
<li>数据驱动：从初始状态出发的正向搜索</li>
<li>目的驱动：从目的状态出发的逆向搜索</li>
<li>双向搜索：两面同时搜索</li>
</ul>
<h3 id="3-一般的解题流程"><a href="#3-一般的解题流程" class="headerlink" title="3. 一般的解题流程"></a>3. 一般的解题流程</h3><p><img src="/2020/06/10/searchAlg1/image-20200610234626613.png" alt="image-20200610234626613"></p>
<h3 id="4-一般的算法"><a href="#4-一般的算法" class="headerlink" title="4. 一般的算法"></a>4. 一般的算法</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. G&#x3D;G0 (G0&#x3D;s), OPEN:&#x3D;(s);</span><br><span class="line">2. CLOSED:&#x3D;( );</span><br><span class="line">3. LOOP: IF OPEN&#x3D;( ) THEN EXIT(FAIL);</span><br><span class="line">4. n:&#x3D;FIRST(OPEN), REMOVE(n, OPEN),</span><br><span class="line">	ADD(n, CLOSED);</span><br><span class="line">5. IF GOAL(n) THEN EXIT(SUCCESS);</span><br><span class="line">6. EXPAND(n)→&#123;mi&#125;, G:&#x3D;ADD(mi, G);</span><br><span class="line">7. 标记和修改指针：</span><br><span class="line">	ADD(mj, OPEN), 并标记mj到n的指针；</span><br><span class="line">	计算是否要修改mk、ml到n的指针；</span><br><span class="line">	计算是否要修改ml到其后继节点的指针；</span><br><span class="line">8. 对OPEN中的节点按某种原则重新排序；</span><br><span class="line">9. GO LOOP；</span><br></pre></td></tr></table></figure>

<h2 id="回溯搜索算法"><a href="#回溯搜索算法" class="headerlink" title="回溯搜索算法"></a>回溯搜索算法</h2><h3 id="1-回溯策略"><a href="#1-回溯策略" class="headerlink" title="1. 回溯策略"></a>1. 回溯策略</h3><p>​        回溯是五大常用算法策略之一，它的核心思想其实就是将解空间看作是一棵树的结构，从树根到其中一个叶子节点的路径就是一个可能的解，根据约束条件，即可得到满足要求的解。求解问题时，发现到某个节点而不满足求解的条件时，就“回溯”返回，尝试别的路径。回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。</p>
<p>​        回溯法师暴力搜索法中的一种，采用试错的思想，尝试分布的去解决一个问题。在分布解决问题的过程中发现得不到正确解答的过程中，将取消上一步或几步的操作，再通过其他方法寻找解决答案。回溯法用最简单的递归的方法来实现。</p>
<h4 id="问题的解空间"><a href="#问题的解空间" class="headerlink" title="问题的解空间"></a>问题的解空间</h4><p>​        在用回溯法解决问题时，应先明确定义问题的解空间。解空间就是一个问题所有的可能性，问题的解空间至少包含问题的一个最优解。</p>
<h4 id="回溯法的基本思想"><a href="#回溯法的基本思想" class="headerlink" title="回溯法的基本思想"></a>回溯法的基本思想</h4><p>​        在确定解空间后，回溯法从根结点出发，以深度优先搜索方式搜索整个解空间。递归的在解空间进行搜索，直到找到所要求的解或解空间中所有解都被遍历。</p>
<blockquote>
<p><strong>回溯法解题的三个步骤：</strong></p>
<ol>
<li>针对所给的问题，定义问题的解空间</li>
<li>确定易于搜索的解空间结构</li>
<li>以深度优先方式搜索解空间，并在搜索过程中调用剪枝函数避免无效搜索。</li>
</ol>
</blockquote>
<h4 id="回溯法的实现"><a href="#回溯法的实现" class="headerlink" title="回溯法的实现"></a>回溯法的实现</h4><ol>
<li>递归回溯</li>
</ol>
<p>使用递归实现回溯法一般函数结构如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 递归回溯函数，t代表当前的递归深度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Bcktract</span><span class="params">(t)</span>:</span></span><br><span class="line">    <span class="comment"># 遍历到解就将解返回</span></span><br><span class="line">    <span class="keyword">if</span> t &gt; n:</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># f(n,t)和g(n,t)分别代表当前节点未处理子树的初始编号和终止编号</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(f(n,t),g(n,t)):</span><br><span class="line">            <span class="comment"># 把当前节点加入返回集合X中</span></span><br><span class="line">            x[t] = h(i)</span><br><span class="line">            <span class="comment"># 如果当前节点满足条件，再在当前节点的基础上进行递归，不满足条件就直接继续下次循环</span></span><br><span class="line">            <span class="comment"># constarint(t) 和 Bound(t)分别为约束函数和限界函数，这两个函数的作用就是剪枝函数</span></span><br><span class="line">            <span class="keyword">if</span> constarint(t) <span class="keyword">and</span> Bound(t):</span><br><span class="line">                Bcktract(t+<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>迭代回溯</li>
</ol>
<p>使用非递归的方式实现回溯法的函数结构如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">IterativeBacktrack</span><span class="params">()</span>:</span></span><br><span class="line">    t = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span>(t &gt; <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 当f(n,t) &lt;= g(n,t)时说明当前节点下一层还没有遍历</span></span><br><span class="line">        <span class="keyword">if</span> (f(n,t) &lt;= g(n,t)):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(f(n,t),g(n,t)):</span><br><span class="line">            	<span class="comment"># 把当前节点加入返回集合X中</span></span><br><span class="line">            	x[t] = h(i)</span><br><span class="line">            	<span class="comment"># 如果当前节点满足条件，再在当前节点的基础上进行递归，不满足条件就直接继续下次循环</span></span><br><span class="line">            	<span class="comment"># constarint(t) 和 Bound(t)分别为约束函数和限界函数，这两个函数的作用就是剪枝函数</span></span><br><span class="line">                <span class="comment"># 两个函数同时满足说明本层的i节点是满足要求的</span></span><br><span class="line">            	<span class="keyword">if</span> constarint(t) <span class="keyword">and</span> Bound(t):</span><br><span class="line">                    <span class="keyword">if</span> (Solution(t)):</span><br><span class="line">                        <span class="keyword">return</span> x</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        t = t+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            t = t - <span class="number">1</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>算法复杂度</li>
</ol>
<p>用回溯法解体的一个显著特征是在搜索过程中动态产生问题的解空间。在任何时刻，算法只保存从根节点到当前节点（扩展节点）的路径。如果解空间树从根节点到叶节点的最长路径的长度为h(n)，则回溯法所需的计算空间通常为O(h(n))。而显式地存储整个解空间则需要O(2^h(n))或O(h(n)!)内存空间。</p>
<h3 id="2-回溯算法存在的问题"><a href="#2-回溯算法存在的问题" class="headerlink" title="2. 回溯算法存在的问题"></a>2. 回溯算法存在的问题</h3><ol>
<li><p>深度问题</p>
<img src="/2020/06/10/searchAlg1/image-20200611231358393.png" alt="image-20200611231358393" style="zoom:80%;">
</li>
<li><p>死循环问题</p>
<p><img src="/2020/06/10/searchAlg1/image-20200611231430823.png" alt="image-20200611231430823"></p>
</li>
<li><p>解决办法</p>
<ul>
<li>对搜索深度加以限制</li>
<li>记录从初始状态到当前状态得路径</li>
</ul>
</li>
</ol>
<h3 id="3-回溯法举例"><a href="#3-回溯法举例" class="headerlink" title="3. 回溯法举例"></a>3. 回溯法举例</h3><p><strong>皇后问题：</strong></p>
<p>​        在4*4的棋盘中，没行放一个棋子，要求每个棋子的是其所在行、所在列、所在斜行的唯一的一个。</p>
<p><strong>解题方法：</strong></p>
<p>​        <img src="/2020/06/10/searchAlg1/search-1.gif" alt="search-1"></p>
<p>算法代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_attack</span><span class="params">(queue, x, y)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x):</span><br><span class="line">        <span class="keyword">if</span> queue[i] == y <span class="keyword">or</span> abs(x - i) == abs(queue[i] - y):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按列来摆放皇后</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">put_position</span><span class="params">(n, queue, col)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_attack(queue, col, i):</span><br><span class="line">            queue[col] = i</span><br><span class="line">            <span class="keyword">if</span> col == n - <span class="number">1</span>:    <span class="comment"># 此时最后一个皇后摆放好了，打印结果。</span></span><br><span class="line">                print(queue)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                put_position(n, queue, col + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">n = <span class="number">4</span>       <span class="comment"># 这里是n 就是n皇后</span></span><br><span class="line"><span class="comment"># 存储皇后位置的一维数组，数组下标表示皇后所在的列，下标对应的值为皇后所在的行。</span></span><br><span class="line">queue = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">put_position(n, queue, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<p><img src="/2020/06/10/searchAlg1/image-20200613094915139.png" alt="image-20200613094915139"></p>
]]></content>
      <categories>
        <category>搜索算法笔记</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>搜索算法</tag>
        <tag>回溯策略</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow框架介绍</title>
    <url>/2020/06/09/tensorflow-1/</url>
    <content><![CDATA[<h3 id="Tensorflow框架介绍"><a href="#Tensorflow框架介绍" class="headerlink" title="Tensorflow框架介绍"></a>Tensorflow框架介绍</h3><h4 id="1-1-TF数据流图"><a href="#1-1-TF数据流图" class="headerlink" title="1.1 TF数据流图"></a>1.1 TF数据流图</h4><h5 id="Tensorflow结构分析"><a href="#Tensorflow结构分析" class="headerlink" title="Tensorflow结构分析"></a>Tensorflow结构分析</h5><blockquote>
<p>构建图阶段:建立流程图，包括定义数据（张量Tensor）和操作（节点Op）</p>
<p>执行图阶段：调用各方资源，将定义好的数据和操作运行起来</p>
</blockquote>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensorflow_dome</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#     原生python加法运算</span></span><br><span class="line">     a = <span class="number">2</span></span><br><span class="line">     b = <span class="number">3</span></span><br><span class="line">     c = a + b</span><br><span class="line">     print(<span class="string">"普通加法结果：\n"</span>,c)</span><br><span class="line">     </span><br><span class="line"><span class="comment">#     tensorflow 加法</span></span><br><span class="line">     a_t = tf.constant(<span class="number">2</span>)</span><br><span class="line">     b_t = tf.constant(<span class="number">3</span>)</span><br><span class="line">     c_t = a_t + b_t</span><br><span class="line">     print(<span class="string">"Tensorflow加法运算的结果：\n"</span>,c_t)</span><br><span class="line">     </span><br><span class="line"><span class="comment">#     开启会话</span></span><br><span class="line">     <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">         c_t_value = sess.run(c_t)</span><br><span class="line">         print(c_t_value)</span><br><span class="line">         </span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609153842522.png" alt="image-20200609153842522"></p>
<h4 id="1-2-图与TensorBoard"><a href="#1-2-图与TensorBoard" class="headerlink" title="1.2 图与TensorBoard"></a>1.2 图与TensorBoard</h4><h5 id="什么是图结构"><a href="#什么是图结构" class="headerlink" title="什么是图结构"></a>什么是图结构</h5><p>图包含了一组tf.Operation代表的计算单元对象和tf.Tensor代表的计算单元之间流动的数据。</p>
<h5 id="图相关操作"><a href="#图相关操作" class="headerlink" title="图相关操作"></a>图相关操作</h5><p> <strong>默认图</strong></p>
<p> 查看默认图的两种方法：</p>
<ul>
<li><p>通过调用tf.get_default_graph()访问，要将操作添加到默认图形中，直接创建OP即可。</p>
</li>
<li><p>op、sess都含有graph属性，默认都在一张图中。</p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">graph_demo</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#    图的演示</span></span><br><span class="line">     a_t = tf.constant(<span class="number">2</span>)</span><br><span class="line">     b_t = tf.constant(<span class="number">3</span>)</span><br><span class="line">     c_t = a_t + b_t</span><br><span class="line">     </span><br><span class="line"><span class="comment">#     查看默认图</span></span><br><span class="line"><span class="comment">#     方法一：调用方法</span></span><br><span class="line">     default_g = tf.get_default_graph()</span><br><span class="line">     print(<span class="string">"default_g:\n"</span>,default_g)</span><br><span class="line"><span class="comment">#     方法二：查看属性</span></span><br><span class="line">     print(<span class="string">"a_t的图属性"</span>,a_t.graph)</span><br><span class="line"></span><br><span class="line"><span class="comment">#     开启会话</span></span><br><span class="line">     <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">         c_t_value = sess.run(c_t)</span><br><span class="line">         print(<span class="string">"c_t_value："</span>,c_t_value)</span><br><span class="line">         print(<span class="string">"sess图属性："</span>,sess.graph)</span><br><span class="line">         </span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p> <strong>创建图</strong></p>
<p> 可以通过tf.Graph()自定义创建图</p>
<p> 举例：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">graph_demo2</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#    自定义图</span></span><br><span class="line">    new_g = tf.Graph()</span><br><span class="line"><span class="comment">#    在图中定义数据和操作</span></span><br><span class="line">    <span class="keyword">with</span> new_g.as_default():</span><br><span class="line">        a_new = tf.constant(<span class="number">20</span>)</span><br><span class="line">        b_new = tf.constant(<span class="number">30</span>)</span><br><span class="line">        c_new = a_new + b_new</span><br><span class="line">        print(<span class="string">"c_new:\n"</span>,c_new)</span><br><span class="line"> <span class="comment">#     开启会话</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session(graph=new_g) <span class="keyword">as</span> new_sess:</span><br><span class="line">        c_new_value = new_sess.run(c_new)</span><br><span class="line">        print(<span class="string">"c_t_value："</span>,c_new_value)</span><br><span class="line">        print(<span class="string">"sess图属性："</span>,new_sess.graph)</span><br></pre></td></tr></table></figure>



<h5 id="TensorBoard：可视化学习"><a href="#TensorBoard：可视化学习" class="headerlink" title="TensorBoard：可视化学习"></a>TensorBoard：可视化学习</h5><p> <strong>1. 数据序列化-events文件</strong></p>
<p> 运行代码将图序列化到本地，生成一个events文件</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.summary.FileWriter(path,graph&#x3D;sess.graph)</span><br></pre></td></tr></table></figure>

<p> <strong>2. 启动TensorBoard</strong></p>
<p> 运行命令生成图，再在浏览器输入127.0.0.1:6006查看。</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir&#x3D;&quot;.&#x2F;tmp&#x2F;summary&#x2F;&quot;</span><br></pre></td></tr></table></figure>



<h5 id="OP（operation）"><a href="#OP（operation）" class="headerlink" title="OP（operation）"></a>OP（operation）</h5><p>常见OP：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609181148718.png" alt="image-20200609181148718"></p>
<p>每一个图一个命名空间</p>
<h4 id="1-3-会话"><a href="#1-3-会话" class="headerlink" title="1.3 会话"></a>1.3 会话</h4><p>会话是一个运行Tensorflow operation的类，开启方式主要包括两种</p>
<ol>
<li><p>tf.Session:用于完整的程序中</p>
</li>
<li><p>tf.InteractiveSession：用于交互式上下文中，例如再cmd中打开运行以后就可以直接交互式的运行。</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609182000166.png" alt="image-20200609182000166"></p>
</li>
</ol>
<h5 id="创建会话初始化的参数、"><a href="#创建会话初始化的参数、" class="headerlink" title="创建会话初始化的参数、"></a>创建会话初始化的参数、</h5><ol>
<li><p>会话掌握资源，用完要回收。</p>
</li>
<li><p>初始化会话对象的参数：</p>
<ul>
<li>graph = None</li>
<li>target：如果设置为空，会话将使用本地计算机中的设备。还可以指定网址，一遍指定tensorflow服务器地址。</li>
<li>config：本参数允许指定一个tf.ConfigProto，以便控制会话的行为。</li>
</ul>
</li>
</ol>
<h5 id="会话的run"><a href="#会话的run" class="headerlink" title="会话的run()"></a>会话的run()</h5><ul>
<li>run(fetches,feed_dict=none,options=None,run_metadata=none)<ul>
<li>通过sess.run()运行operation</li>
<li>detches：单一的operation，或者列表、元祖</li>
<li>feed_dict：与tf.placeholder配合使用，运行时赋值使用<ul>
<li>placeholder提供占位符，run的时候通过feed_dict指定参数</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_demo</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#    图的演示</span></span><br><span class="line">     a_t = tf.placeholder(tf.float32)</span><br><span class="line">     b_t = tf.placeholder(tf.float32)</span><br><span class="line">     c_t = a_t + b_t</span><br><span class="line"></span><br><span class="line"><span class="comment">#     开启会话</span></span><br><span class="line">     <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">         c_t_value = sess.run(c_t,feed_dict=&#123;a_t:<span class="number">1</span>,b_t:<span class="number">2</span>&#125;)</span><br><span class="line">         print(<span class="string">"c_t_value："</span>,c_t_value)</span><br></pre></td></tr></table></figure>

<p>运行结果</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609203435131.png" alt="image-20200609203435131"></p>
<h4 id="1-4-张量"><a href="#1-4-张量" class="headerlink" title="1.4 张量"></a>1.4 张量</h4><h5 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h5><p>tensorflow就是一个n维数组，类型为tf.Tensor。</p>
<blockquote>
<p>张量在计算机中怎么存储？</p>
<p>标量  一个数字</p>
<p>数组  一维数组</p>
<p>矩阵  二维数组</p>
<p>张量  n维数组</p>
</blockquote>
<p><strong>张量的类型</strong></p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609211541820.png" alt="image-20200609211541820"></p>
<p><strong>张量的阶</strong></p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609211628549.png" alt="image-20200609211628549"></p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensor_demo</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#    张量的演示</span></span><br><span class="line">    tensor1 = tf.constant(<span class="number">4.0</span>)</span><br><span class="line">    tensor2 = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">    linear_square = tf.constant([[<span class="number">4</span>],[<span class="number">9</span>],[<span class="number">16</span>],[<span class="number">25</span>]],dtype=tf.int32)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"tensor1:"</span>,tensor1)</span><br><span class="line">    print(<span class="string">"tensor2:"</span>,tensor2)</span><br><span class="line">    print(<span class="string">"linear_square:"</span>,linear_square)</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609212425728.png" alt="image-20200609212425728"></p>
<p><strong>张量的变换</strong></p>
<ul>
<li>类型改变</li>
</ul>
<p><img src="/2020/06/09/tensorflow-1/image-20200609230835007.png" alt="image-20200609230835007"></p>
<blockquote>
<p>和ndarray属性修改对比：</p>
<ol>
<li><p>ndarray.astype(type)</p>
<p>tf.cast(tensor,dtype)</p>
<p>​    不会修改原始的tensor，返回新的改变类型后的tensor</p>
</li>
<li><p>ndarray.tostring</p>
</li>
</ol>
</blockquote>
<ul>
<li><p>形状改变</p>
<ul>
<li><p>静态形状：初始创建张量时的形状</p>
<p>如何改变静态形状？</p>
<blockquote>
<p>什么情况下才可以改变/更新静态形状？</p>
<p>在形状还没有完全固定下来额时候 </p>
</blockquote>
<p>tensor.set_shape()</p>
<p>只能更新形状没确定部分。</p>
</li>
<li><p>动态形状</p>
<ul>
<li>可能随意改变形状，但是数据总量不能改变。</li>
<li>tf.reshape(tensor,shape )<ul>
<li>不会改变原始的tensor</li>
<li>返回新的改变形状后的tensor</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a_t = tf.placeholder(tf.float32,shape=[<span class="literal">None</span>,<span class="literal">None</span>])</span><br><span class="line">    b_t = tf.placeholder(tf.float32,shape=[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line"><span class="comment">#    形状确定的</span></span><br><span class="line">    c_t = tf.placeholder(tf.float32,shape=[<span class="number">3</span>,<span class="number">2</span>])</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"a_t:"</span>,a_t)</span><br><span class="line">    print(<span class="string">"b_t:"</span>,b_t)</span><br><span class="line">    print(<span class="string">"c_t:"</span>,c_t)</span><br><span class="line"><span class="comment">#   修改静态形状</span></span><br><span class="line">    b_t.set_shape([<span class="number">2</span>,<span class="number">10</span>])</span><br><span class="line"><span class="comment">#    修改动态形状</span></span><br><span class="line">    tf.reshape(c_t,shape=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line">    print(<span class="string">"b_t:"</span>,b_t)</span><br><span class="line">    print(<span class="string">"c_t:"</span>,c_t)</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609235623814.png" alt="image-20200609235623814"></p>
<h4 id="1-5-API"><a href="#1-5-API" class="headerlink" title="1.5 API"></a>1.5 API</h4><h5 id="基础API"><a href="#基础API" class="headerlink" title="基础API"></a>基础API</h5><ul>
<li><strong>tf.app</strong>:相当于为tensorflow提供的一个main函数</li>
<li><strong>tf.image</strong>:图像处理的操作。</li>
<li><strong>tf.gfile</strong>:文件操作函数。</li>
<li><strong>tf.summary</strong>:用来生成TensorBoard可用的统计日志</li>
<li><strong>tf.python_io</strong>:数据读取</li>
<li><strong>tf.train</strong>：提供一些训练器</li>
<li><strong>tf.nn</strong>:一些构建神经网络的底层函数</li>
</ul>
<h5 id="高级API"><a href="#高级API" class="headerlink" title="高级API"></a>高级API</h5><p><strong>tf.keras</strong>:本来是一个独立的深度学习库，tf用来快速构建模型。</p>
<p><strong>tf.layers</strong>:以更高级的概念定义一个模型</p>
<p><strong>tf.contrib</strong>：构建计算图的高级操作。</p>
<p><strong>tf.estimator</strong>:相当于构建模型、训练、评价的合体</p>
<h5 id="api层次划分"><a href="#api层次划分" class="headerlink" title="api层次划分"></a>api层次划分</h5><p><img src="/2020/06/09/tensorflow-1/image-20200610072817901.png" alt="image-20200610072817901"></p>
<h4 id="1-6-案例：实现线性回归的训练"><a href="#1-6-案例：实现线性回归的训练" class="headerlink" title="1.6 案例：实现线性回归的训练"></a>1.6 案例：实现线性回归的训练</h4><p>一次函数的线性回归，随机指定一百个点</p>
<p>代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_regression</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#    实现一个线性回归</span></span><br><span class="line"><span class="comment">#    1. 准备数据</span></span><br><span class="line">    X = tf.random_normal(shape=[<span class="number">100</span>,<span class="number">1</span>])</span><br><span class="line">    y_true = tf.matmul(X,[[<span class="number">0.8</span>]]) + <span class="number">0.6</span></span><br><span class="line"><span class="comment">#    2.构建模型</span></span><br><span class="line">    weight = tf.Variable(initial_value=tf.random_normal(shape=[<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line">    bias = tf.Variable(initial_value=tf.random_normal(shape=[<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line">    y_predict = tf.matmul(X,weight) + bias</span><br><span class="line"><span class="comment">#    3.构建损失函数</span></span><br><span class="line">    error = tf.reduce_mean(tf.square(y_predict - y_true))</span><br><span class="line"><span class="comment">#    4.优化损失</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(error)</span><br><span class="line">    </span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init)</span><br><span class="line">        print(<span class="string">"训练前模型参数为：权重%f 偏置%f 损失%f"</span>%(weight.eval(),bias.eval(),error.eval()))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">            sess.run(optimizer)</span><br><span class="line">        print(<span class="string">"训练后模型参数为：权重%f 偏置%f 损失%f"</span>%(weight.eval(),bias.eval(),error.eval()))</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610102402102.png" alt="image-20200610102402102"></p>
<h4 id="1-7-需要注意的问题"><a href="#1-7-需要注意的问题" class="headerlink" title="1.7 需要注意的问题"></a>1.7 需要注意的问题</h4><h5 id="学习率的设置、步数的设置与梯度爆炸"><a href="#学习率的设置、步数的设置与梯度爆炸" class="headerlink" title="学习率的设置、步数的设置与梯度爆炸"></a>学习率的设置、步数的设置与梯度爆炸</h5><p>学习率越大，训练到较好结果的步数越小；学习率越小，训练到较好结果的步数越大。</p>
<p>学习率过大会导致梯度爆炸的情况。</p>
<blockquote>
<p>在极端情况下，权重的值变得非常大，以至于溢出导致NaN.</p>
<p>如何解决梯度爆炸问题？</p>
<ol>
<li>重新设计网络</li>
<li>调整学习率</li>
<li>使用梯度阶段（在训练过程中检查和限制梯度的大小）</li>
<li>使用激活函数</li>
</ol>
</blockquote>
<p><strong>trainable属性</strong></p>
<p>在定义变量时属性可以规定本trainable数据是否可以训练。</p>
<h4 id="1-8-案例改进"><a href="#1-8-案例改进" class="headerlink" title="1.8 案例改进"></a>1.8 案例改进</h4><p>对案例在三方面进行了改进，分别是增加变量显示，增加命名空间、模型的保存与加载。</p>
<ul>
<li><p>增加变量显示</p>
<ol>
<li>创建事件文件</li>
<li>收集变量</li>
<li>合并变量</li>
<li>每次迭代运行一次合并变量</li>
<li>每次迭代将summary对象写入事件文件</li>
</ol>
</li>
<li><p>增加命名空间</p>
<ul>
<li>为每个部分增加命名空间可以在tensorboard里更清楚地观察模型</li>
</ul>
</li>
<li><p>模型的保存与加载</p>
<ol>
<li><p>实例化Saver</p>
</li>
<li><p>保存（在训练迭代过程中对会话进行保存）</p>
<p>saver.save(sess,path)</p>
</li>
<li><p>加载（对会话进行加载）</p>
<p>先判断模型是否存在，存在再进行加载。</p>
</li>
</ol>
</li>
</ul>
<p>代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_regression</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#    实现一个线性回归</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"prepare_data"</span>):</span><br><span class="line">    <span class="comment">#    1. 准备数据</span></span><br><span class="line">        X = tf.random_normal(shape=[<span class="number">100</span>,<span class="number">1</span>],name=<span class="string">"feature"</span>)</span><br><span class="line">        y_true = tf.matmul(X,[[<span class="number">0.8</span>]]) + <span class="number">0.6</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"create_model"</span>):</span><br><span class="line">    <span class="comment">#    2.构建模型</span></span><br><span class="line">        weight = tf.Variable(initial_value=tf.random_normal(shape=[<span class="number">1</span>,<span class="number">1</span>]),name=<span class="string">"Weights"</span>)</span><br><span class="line">        bias = tf.Variable(initial_value=tf.random_normal(shape=[<span class="number">1</span>,<span class="number">1</span>]),name=<span class="string">"bias"</span>)</span><br><span class="line">        y_predict = tf.matmul(X,weight) + bias</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"loss_function"</span>):</span><br><span class="line">    <span class="comment">#    3.构建损失函数</span></span><br><span class="line">        error = tf.reduce_mean(tf.square(y_predict - y_true))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"optimizer"</span>):</span><br><span class="line">    <span class="comment">#    4.优化损失</span></span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(error)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#    2_收集变量</span></span><br><span class="line"><span class="comment">#    scalar用来收集</span></span><br><span class="line">    tf.summary.scalar(<span class="string">"error"</span>,error)</span><br><span class="line">    tf.summary.histogram(<span class="string">"weights"</span>,weight)</span><br><span class="line">    tf.summary.histogram(<span class="string">"bias"</span>,bias)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#    3_合并变量</span></span><br><span class="line">    merged = tf.summary.merge_all()</span><br><span class="line"> </span><br><span class="line"><span class="comment">#    创建实例化Saver</span></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#    初始化所有数据</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#    开始会话</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"><span class="comment">#        初始化变量</span></span><br><span class="line">        sess.run(init)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#        1_创建事件文件</span></span><br><span class="line">        file_writer = tf.summary.FileWriter(<span class="string">"./tmp/linear"</span>,graph=sess.graph)</span><br><span class="line">        </span><br><span class="line">        print(<span class="string">"训练前模型参数为：权重%f 偏置%f 损失%f"</span>%(weight.eval(),bias.eval(),error.eval()))</span><br><span class="line"><span class="comment">#        开始训练</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            sess.run(optimizer)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 运行合并操作</span></span><br><span class="line">            summary = sess.run(merged)</span><br><span class="line">            <span class="comment"># 将每次迭代后的</span></span><br><span class="line">            file_writer.add_summary(summary,i)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 保存模型</span></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                saver.save(sess,<span class="string">"./tmp/model/my_linear.ckpt"</span>)</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        print(<span class="string">"训练后模型参数为：权重%f 偏置%f 损失%f"</span>%(weight.eval(),bias.eval(),error.eval()))</span><br></pre></td></tr></table></figure>

<p>增加命名空间后生成的tensorboard图：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610171639402.png" alt="image-20200610171639402"></p>
<p>模型保存后生成的文件如图，里面没有具体的ckpt文件</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610171914326.png" alt="image-20200610171914326"></p>
<p>加载模型只需把原有训练模型的位置改为下面的代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> os.path.exists(<span class="string">"./tmp/model/checkpoint"</span>):</span><br><span class="line">    saver.restore(sess,<span class="string">"./tmp/model/my_linear.ckpt"</span>)</span><br></pre></td></tr></table></figure>

<p>加载后运行结果：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610172424010.png" alt="image-20200610172424010"></p>
<h4 id="1-9-命令行参数使用"><a href="#1-9-命令行参数使用" class="headerlink" title="1.9 命令行参数使用"></a>1.9 命令行参数使用</h4><ol>
<li><p>通过tf.app.flags定义从命令行接受的参数</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610174302834.png" alt="image-20200610174302834"></p>
</li>
<li><p>简化变量名</p>
</li>
</ol>
<p>例子：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610174432963.png" alt="image-20200610174432963"></p>
<p>运行：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610174450754.png" alt="image-20200610174450754"></p>
<p>命令行运行：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610174536346.png" alt="image-20200610174536346"></p>
<blockquote>
<p><strong>tf.app.run函数</strong></p>
<p>tf.app.run()会自动调用程序中的main(argv)函数</p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>学习笔记</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>基于矩阵操作的必然性QoS约束副本放置方法 笔记</title>
    <url>/2020/06/06/paperNote1/</url>
    <content><![CDATA[<blockquote>
<p>论文名称：基于矩阵操作的必然性QoS约束副本放置方法</p>
<p>作者：付伟，叶清，吴晓平</p>
<p>发表时间：2012.12</p>
<p>发表期刊：系统工程理论实践</p>
</blockquote>
<h2 id="阅读笔记"><a href="#阅读笔记" class="headerlink" title="阅读笔记"></a>阅读笔记</h2><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h3><p>这篇论文讲的就是用矩阵计算的方法在满足必然性约束的条件下怎么在分布式系统中进行副本放置。</p>
<h4 id="1-1-副本放置问题"><a href="#1-1-副本放置问题" class="headerlink" title="1.1 副本放置问题"></a>1.1 副本放置问题</h4><h5 id="副本技术"><a href="#副本技术" class="headerlink" title="副本技术"></a>副本技术</h5><blockquote>
<p>副本技术是应用在分布式系统中，把服务器中的数据通过在其他机器中建立副本的方式，用来提升系统的整体性能。副本技术通过牺牲一定的存储开销和一致性维护开销，方便用户以“就近原则”访问共享数据资源，从而使系统性能全面提升。</p>
</blockquote>
<h5 id="副本放置问题"><a href="#副本放置问题" class="headerlink" title="副本放置问题"></a>副本放置问题</h5><blockquote>
<p>副本放置问题是副本技术要解决的核心问题，决定副本系统中需要多少个副本和这些副本如何分布，从而能够达到某种性能指标或者系统设计要求。</p>
</blockquote>
<h4 id="1-2-必然性QoS约束"><a href="#1-2-必然性QoS约束" class="headerlink" title="1.2 必然性QoS约束"></a>1.2 必然性QoS约束</h4><h5 id="QoS-服务质量"><a href="#QoS-服务质量" class="headerlink" title="QoS(服务质量)"></a>QoS(服务质量)</h5><blockquote>
<p>服务质量描述一个服务满足客户需求的能力，是服务好坏的定量度量。</p>
</blockquote>
<h5 id="或然性服务质量约束（总体服务质量约束）"><a href="#或然性服务质量约束（总体服务质量约束）" class="headerlink" title="或然性服务质量约束（总体服务质量约束）"></a>或然性服务质量约束（总体服务质量约束）</h5><blockquote>
<p>从系统的全局出发，将服务的总体指标作为QoS评价的依据。以全局服务质量指标作为优化目标。</p>
</blockquote>
<h5 id="必然性服务质量约束（个体服务质量约束）"><a href="#必然性服务质量约束（个体服务质量约束）" class="headerlink" title="必然性服务质量约束（个体服务质量约束）"></a>必然性服务质量约束（个体服务质量约束）</h5><blockquote>
<p>每个用户的QoS请求都100%的得到满足。比如在股票市场上, 股东需要实时获得股票信息当股票信息发生变化时, 每个股东都必须在第一时间内获取到最新的股票价格 如果因为股票信息系统的原因导致某个股东在经过一段时间延迟之后才获得所需数据, 这将使其错过股票交易的最佳时机, 从而可能蒙受重大经济损失。</p>
</blockquote>
<h5 id="两种服务质量约束举例说明"><a href="#两种服务质量约束举例说明" class="headerlink" title="两种服务质量约束举例说明"></a>两种服务质量约束举例说明</h5><p>总体服务质量约束和个体服务质量约束都是对服务质量的要求。如图片所示是三种客户节点访问服务器的情况。第一个是未设置副本节点的情况下客户节点访问服务器，此时系统的总消耗是45，平均每个节点的消耗是4.5。第二个是在满足总体服务质量约束的条件下客户节点访问服务器节点和副本节点，此时系统的总消耗是13，系统的平均消耗是1.3，此时是放置一个副本情况下平均开销最小的情况，也就是满足总体服务质量约束最好的情况。第三个是满足所有客户节点访问服务的消耗都不大于3的个体服务质量约束的条件下客户节点的消耗，此时系统的总开销15，平均开销1.5，单个节点消耗的最大值是3，满足个体服务质量约束。第二个虽然整体的平均开销情况最好，但是不满足个体服务质量约束。</p>
<p><img src="/2020/06/06/paperNote1/image-20200606215837566.png" alt="image-20200606215837566"></p>
<h5 id="必然性约束的表示"><a href="#必然性约束的表示" class="headerlink" title="必然性约束的表示"></a>必然性约束的表示</h5><blockquote>
<p> 所用到的符号的说明</p>
<p><img src="/2020/06/06/paperNote1/image-20200606230820213.png" alt="image-20200606230820213"></p>
<p><strong>QoS属性满足</strong></p>
<p><img src="/2020/06/06/paperNote1/image-20200606231118247.png" alt="image-20200606231118247"></p>
<p><strong>QoS请求差额</strong></p>
<p><img src="/2020/06/06/paperNote1/C:%5CHexo%5Csource_posts%5CpaperNote1%5Cimage-20200606231303225.png" alt="image-20200606231303225"></p>
<p><strong>必然性约束</strong></p>
<p><img src="/2020/06/06/paperNote1/image-20200606231919366.png" alt="image-20200606231919366"></p>
<p><strong>必然性约束满足</strong></p>
<p>​        如果必然性约束QoS_Certain(S) = 0，则称服务S是必然性约束满足的。</p>
</blockquote>
<h4 id="1-3-矩阵操作"><a href="#1-3-矩阵操作" class="headerlink" title="1.3 矩阵操作"></a>1.3 矩阵操作</h4><h5 id="矩阵最小并"><a href="#矩阵最小并" class="headerlink" title="矩阵最小并"></a>矩阵最小并</h5><p>对于两个形状相同的矩阵X、Y，两个矩阵的矩阵最小并记作Z=X△Y，Z等于X和Y中对应每个位置元素的较小值。</p>
<h5 id="矩阵正相差"><a href="#矩阵正相差" class="headerlink" title="矩阵正相差"></a>矩阵正相差</h5><p>对于两个形状相同的矩阵X、Y，两个矩阵的矩阵正相差记作Z=X▽Y，其中Z中每个位置元素的值如图所示。</p>
<p><img src="/2020/06/06/paperNote1/image-20200606233216842.png" alt="image-20200606233216842"></p>
<h5 id="矩阵和"><a href="#矩阵和" class="headerlink" title="矩阵和"></a>矩阵和</h5><p>矩阵和就是矩阵中所有元素的代数和。</p>
<p><img src="/2020/06/06/paperNote1/image-20200606233342102.png" alt="image-20200606233342102"></p>
<h4 id="1-4-矩阵表示"><a href="#1-4-矩阵表示" class="headerlink" title="1.4 矩阵表示"></a>1.4 矩阵表示</h4><p>假设系统中包含n个节点，每个节点的QOS约束包括m个元素。</p>
<h5 id="质量约束矩阵-BM"><a href="#质量约束矩阵-BM" class="headerlink" title="质量约束矩阵(BM)"></a>质量约束矩阵(BM)</h5><p>质量约束矩阵中存储每个节点的每个属性的QoS需求。</p>
<p><img src="/2020/06/06/paperNote1/image-20200606234233744.png" alt="image-20200606234233744"></p>
<h5 id="服务质量矩阵（QMj）"><a href="#服务质量矩阵（QMj）" class="headerlink" title="服务质量矩阵（QMj）"></a>服务质量矩阵（QMj）</h5><p>服务质量矩阵表示当j节点单独提供服务时每个节点所获得的不同属性的QoS质量。</p>
<p><img src="/2020/06/06/paperNote1/paperNote1%5Cimage-20200606234452406.png" alt="image-20200606234452406"></p>
<h5 id="加权向量（WV）"><a href="#加权向量（WV）" class="headerlink" title="加权向量（WV）"></a>加权向量（WV）</h5><p>加权向量是指每个属性在QoS约束中所占的比重。</p>
<p><img src="/2020/06/06/paperNote1/image-20200606234618642.png" alt="image-20200606234618642"></p>
<h5 id="服务质量矩阵（DM）"><a href="#服务质量矩阵（DM）" class="headerlink" title="服务质量矩阵（DM）"></a>服务质量矩阵（DM）</h5>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>副本放置</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯网络3 贝叶斯网络</title>
    <url>/2020/06/04/bayes-3/</url>
    <content><![CDATA[<h2 id="贝叶斯计算"><a href="#贝叶斯计算" class="headerlink" title="贝叶斯计算"></a>贝叶斯计算</h2><h3 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h3><h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><h5 id="独立"><a href="#独立" class="headerlink" title="独立"></a>独立</h5><blockquote>
<p>事件X和Y相互独立，则</p>
<ul>
<li><p>P(X,Y) = P(X)P(Y)</p>
</li>
<li><p>P(X|Y) = P(X)</p>
</li>
</ul>
</blockquote>
<h5 id="条件独立"><a href="#条件独立" class="headerlink" title="条件独立"></a>条件独立</h5><blockquote>
<p>如果在给定Z的条件下，X和Y相互独立</p>
<ul>
<li>P(X|Y,Z) = P(X|Z)</li>
</ul>
<p>就是Z条件下X的概率不受Y的影响</p>
</blockquote>
<h5 id="联合概率"><a href="#联合概率" class="headerlink" title="联合概率"></a>联合概率</h5><blockquote>
<p>P(X1,X2…Xn)表示X1,X2…Xn同时发生的概率</p>
<p>如果X1,X2…Xn相互独立：</p>
<ul>
<li><em>P</em>(<em>X</em>1, <em>X</em>2, …, Xn) = <em>P</em>(<em>X</em>1) <em>P</em>(<em>X</em>2) …<em>P</em>(Xn)</li>
</ul>
<p>条件概率：</p>
<ul>
<li><em>P</em>(<em>X</em>1, <em>X</em>2, …, Xn) = <em>P</em>(<em>X</em>1|<em>X</em>2, …,Xn) <em>P</em>(<em>X</em>2, …, Xn)</li>
</ul>
<p>迭代表示：</p>
<ul>
<li>​     <em>P</em>(<em>X</em>1, <em>X</em>2, …, Xn)</li>
<li>​            = <em>P</em>(<em>X</em>1) <em>P</em>(<em>X</em>2| <em>X</em>1) <em>P</em>(<em>X</em>3| <em>X</em>2<em>X</em>1)…<em>P</em>(Xn|Xn-1, …, X1)</li>
<li>​            = <em>P</em>(<em>X**N</em>) <em>P</em>(<em>X**N</em>-1| <em>X**N</em>) <em>P</em>(<em>X**N</em>-2| <em>X**N</em>-1<em>X**N</em>)…<em>P</em>(<em>X</em>1|<em>X</em>2, …, <em>X**N</em>)</li>
</ul>
</blockquote>
<h5 id="贝叶斯公式："><a href="#贝叶斯公式：" class="headerlink" title="贝叶斯公式："></a>贝叶斯公式：</h5><h4 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h4><p><img src="/2020/06/04/bayes-3/image-20200604093939830.png" alt="image-20200604093939830"></p>
<h3 id="主观贝叶斯"><a href="#主观贝叶斯" class="headerlink" title="主观贝叶斯"></a>主观贝叶斯</h3><h4 id="公式-1"><a href="#公式-1" class="headerlink" title="公式"></a>公式</h4><h5 id="规则的不确定性"><a href="#规则的不确定性" class="headerlink" title="规则的不确定性"></a>规则的不确定性</h5><blockquote>
<p>LS：表示A为真时对B的影响。</p>
<p><img src="/2020/06/04/bayes-3/image-20200604094349083.png" alt="image-20200604094349083"></p>
<p>LN：表示A为假时对B的影响。(LN表示的就是规则的不确定性，贝叶斯网络中只考虑了规则的确定性，没考虑当A为假的情况)</p>
<p><img src="/2020/06/04/bayes-3/image-20200604094422795.png" alt="image-20200604094422795"></p>
</blockquote>
<h4 id="例题-1"><a href="#例题-1" class="headerlink" title="例题"></a>例题</h4>]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>贝叶斯网络</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯网络2 贝叶斯公式</title>
    <url>/2020/05/31/bayes-2/</url>
    <content><![CDATA[<h2 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h2>]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>贝叶斯网络</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯网络1 概率基础</title>
    <url>/2020/05/31/nayes-1/</url>
    <content><![CDATA[<h2 id="概率基础"><a href="#概率基础" class="headerlink" title="概率基础"></a>概率基础</h2><h3 id="随机试验和随机事件"><a href="#随机试验和随机事件" class="headerlink" title="随机试验和随机事件"></a>随机试验和随机事件</h3><p>所涉及的基本概念：</p>
<blockquote>
<p>试验：为了查看某件事的结果或某物的性能而从事的某种活动。</p>
<p>随机试验（简称试验）：满足如下几个条件的试验就是随机试验：</p>
<blockquote>
<p>可重复性：在相同条件下可重复进行</p>
<p>不确定性：每次试验前不能准确预测哪一个结果会发生</p>
<p>可观察性：每次试验结果可能不止一个，并且事先知道所有可能结果</p>
</blockquote>
<p>基本事件（样本点）：每一种可能出现的情况</p>
<p>样本空间：所有样本点的集合</p>
<p>随机事件（简称事件）：由基本事件复合而成的事件</p>
<p>必然事件：一定会发生的事件</p>
<p>不可能事件：一定不会发生的事件</p>
<p>完备事件组：一组事件且两两没有交集，所有事件并起来就是样本空间</p>
</blockquote>
<p>用韦恩图进行表示：</p>
<p><img src="/2020/05/31/nayes-1/image-20200531151801359.png" alt="image-20200531151801359"></p>
<h3 id="概率及其性质"><a href="#概率及其性质" class="headerlink" title="概率及其性质"></a>概率及其性质</h3><ul>
<li><p>概率用来描述随机事件发生的可能性的大小，值在0到1之间。</p>
</li>
<li><p>性质：</p>
<p><img src="/2020/05/31/nayes-1/image-20200531160806048.png" alt="image-20200531160806048"></p>
</li>
</ul>
<h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><p><img src="/2020/05/31/nayes-1/image-20200531160938552.png" alt="image-20200531160938552"></p>
<h4 id="事件独立"><a href="#事件独立" class="headerlink" title="事件独立"></a>事件独立</h4><blockquote>
<p> <strong>定义：</strong></p>
<p>如果两个事件事件是否发生互相没有影响，称两个事件互相独立</p>
<p>设A、B两个事件，B发生的可能性不受到A的影响，即P(B|A)=P(B)，则A B两个事件相互独立</p>
</blockquote>
<blockquote>
<p><strong>充要条件：</strong></p>
<p>P(AB) = P(A)P(B)</p>
</blockquote>
<blockquote>
<p><strong>互不相容和互相独立</strong>：</p>
<ul>
<li><p>互不相容是两个事件没有交集</p>
</li>
<li><p>相互独立是两个事件没有影响</p>
</li>
<li><p>没有交集的两个事件也可能是有影响的</p>
</li>
</ul>
</blockquote>
<h3 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h3><p>全概率定理：如果事件A1、A2 … An是一个完备事件组，并且都有正概率，则有：</p>
<p>P(B) = P(A_{1})P(B|A_{1}) + P(A_{2})P(B|A_{2}) + … + P(A_{n})P(B|A_{n})</p>
<p> 一个复杂的概率事件问题可以转化为不同情况或者不同原因下发生的简单事件的概率求和问题。</p>
<p><img src="/2020/05/31/nayes-1/image-20200531164456756.png" alt="image-20200531164456756"></p>
<p>举例：</p>
<p><img src="/2020/05/31/nayes-1/image-20200531164542739.png" alt="image-20200531164542739"></p>
]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>贝叶斯网络</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode-3</title>
    <url>/2020/05/29/leetcode-3/</url>
    <content><![CDATA[<h2 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h2><blockquote>
<p>题目编号：198</p>
<p>题目标题：打家劫舍</p>
<p>题目难度： 简单</p>
<p>说明：</p>
<p>你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。</p>
<p>给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。</p>
<p>示例1：</p>
<blockquote>
<p>输入：[1,2,3,1]</p>
<p>输出：4</p>
</blockquote>
<p>示例2：</p>
<blockquote>
<p>输入：[2,7,9,3,1]</p>
<p>输出：12</p>
</blockquote>
</blockquote>
<h2 id="题目解答"><a href="#题目解答" class="headerlink" title="题目解答"></a>题目解答</h2><h3 id="方法-动态规划"><a href="#方法-动态规划" class="headerlink" title="方法    动态规划"></a>方法    动态规划</h3><p>思路：</p>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>简单难度</tag>
        <tag>动态规划</tag>
        <tag>滚动数组</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络笔记2 神经网络模型</title>
    <url>/2020/05/28/networkModel/</url>
    <content><![CDATA[<h2 id="神经网络笔记2-–-神经网络模型"><a href="#神经网络笔记2-–-神经网络模型" class="headerlink" title="神经网络笔记2 – 神经网络模型"></a>神经网络笔记2 – 神经网络模型</h2><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>神经网络的具体分类如图：</p>
<p><img src="/2020/05/28/networkModel/1.png" alt="分类"></p>
<p>各模型结构：</p>
<blockquote>
<p>层次模型中单纯层次结构、层内有互联、输出层到输入层有连接结构如图：</p>
<p><img src="/2020/05/28/networkModel/2.png" alt="层次模型"></p>
<p>互连模型中全互连和局部互连模型如图：</p>
<p><img src="/2020/05/28/networkModel/3.png" alt="互联模型"></p>
<p>前馈性网络和反馈性网络的结构如图：</p>
<p><img src="/2020/05/28/networkModel/4.png" alt="前馈网络和反馈网络"></p>
</blockquote>
<h3 id="前馈神经网络和反馈神经网络"><a href="#前馈神经网络和反馈神经网络" class="headerlink" title="前馈神经网络和反馈神经网络"></a>前馈神经网络和反馈神经网络</h3><p><strong>前馈神经网络：</strong></p>
<blockquote>
<p>前馈神经网络是一种最简单的神经网络，采用单向多层结构。</p>
<p><img src="/2020/05/28/networkModel/5.png" alt="前馈神经网络结构"></p>
</blockquote>
<p>反馈神经网络：</p>
<blockquote>
<p>反馈神经网络将输出再返回到输入进行训练</p>
<p>常见的反馈神经网络：Hopfield神经网络、Elman神经网络、Boltzmann</p>
</blockquote>
<p><strong>前馈神经网络与反馈神经网络的结构</strong>：</p>
<blockquote>
<p><img src="/2020/05/28/networkModel/6.png" alt="结构"></p>
</blockquote>
<p><strong>前馈神经网络与反馈神经网络的结构</strong></p>
<blockquote>
<p><img src="/2020/05/28/networkModel/7.png" alt="区别"></p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode-2</title>
    <url>/2020/05/28/leetcode-2/</url>
    <content><![CDATA[<h2 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h2><blockquote>
<p>题目编号：136</p>
<p>题目标题：只出现一次的数字</p>
<p>题目难度： 简单</p>
<p>说明：</p>
<p>给定一个<strong>非空</strong>整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。</p>
<ul>
<li>算法具有线性时间复杂度。</li>
<li>不使用额外的空间。</li>
</ul>
<p>示例1：</p>
<blockquote>
<p>输入：[2,2,1]</p>
<p>输出：1</p>
</blockquote>
<p>示例2：</p>
<blockquote>
<p>输入：[4,1,2,1,2]</p>
<p>输出：4</p>
</blockquote>
</blockquote>
<h2 id="题目解答"><a href="#题目解答" class="headerlink" title="题目解答"></a>题目解答</h2><h3 id="方法一-异或"><a href="#方法一-异或" class="headerlink" title="方法一     异或"></a>方法一     异或</h3><h4 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h4><blockquote>
<ul>
<li>两个相同数字的异或结果为0，对所有数字进行异或运算，所有相同的数字都相互抵消，最后剩下的就是出现一次的数字。</li>
<li>因为异或运算满足交换率和结合率，所以数字顺序不影响运算。</li>
<li><img src="/2020/05/28/leetcode-2/0.png" alt="交换"></li>
</ul>
</blockquote>
<h4 id="图示-："><a href="#图示-：" class="headerlink" title="图示 ："></a>图示 ：</h4><p><img src="/2020/05/28/leetcode-2/1.png" alt="异或"></p>
<h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">singleNumber</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">        a = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            a ^= i</span><br><span class="line">        <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure>



<h3 id="方法二-数学计算"><a href="#方法二-数学计算" class="headerlink" title="方法二    数学计算"></a>方法二    数学计算</h3><h4 id="思路：-1"><a href="#思路：-1" class="headerlink" title="思路："></a>思路：</h4><blockquote>
<p>先通过set把数据去重，然后把所有的值相加*2去减之前的值，剩下的值就是答案</p>
</blockquote>
<h4 id="代码：-1"><a href="#代码：-1" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">singleNumber</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span> * sum(set(nums)) - sum(nums)</span><br></pre></td></tr></table></figure>



<h3 id="方法三-Counter"><a href="#方法三-Counter" class="headerlink" title="方法三     Counter"></a>方法三     Counter</h3><h4 id="代码：-2"><a href="#代码：-2" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">singleNumber</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        datas = Counter(nums)</span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> datas:</span><br><span class="line">            <span class="keyword">if</span> datas[each] == <span class="number">1</span>: <span class="keyword">return</span> each</span><br></pre></td></tr></table></figure>

<h3 id="方法四-数组切片（费时）"><a href="#方法四-数组切片（费时）" class="headerlink" title="方法四     数组切片（费时）"></a>方法四     数组切片（费时）</h3><h4 id="代码：-3"><a href="#代码：-3" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def singleNumber(self, nums: List[int]) -&gt; int:</span><br><span class="line">        for i in range(len(nums)):</span><br><span class="line">            if nums[i] not in nums[0:i] and nums[i] not in nums[i+1:]: return nums[i]</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>简单难度</tag>
        <tag>位运算</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode-1</title>
    <url>/2020/05/27/leetcode-1/</url>
    <content><![CDATA[<h2 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h2><blockquote>
<p>题目编号：350</p>
<p>题目标题：两个数组的交集 Ⅱ</p>
<p>说明：</p>
<ul>
<li>输出结果中每个元素出现的次数，应与元素在两个数组中出现的次数一致。</li>
<li>我们可以不考虑输出结果的顺序。</li>
</ul>
<p>示例1：</p>
<blockquote>
<p>输入：nums1 = [1,2,2,1], nums2 = [2,2]</p>
<p>输出：[2,2]</p>
</blockquote>
<p>示例2：</p>
<blockquote>
<p>输入：nums1 = [4,9,5], nums2 = [9,4,9,8,4]</p>
<p>输出：[4,9]</p>
</blockquote>
</blockquote>
<h2 id="题目解答"><a href="#题目解答" class="headerlink" title="题目解答"></a>题目解答</h2><h3 id="方法一-递归调用（自己写的算法）"><a href="#方法一-递归调用（自己写的算法）" class="headerlink" title="方法一     递归调用（自己写的算法）"></a>方法一     递归调用（自己写的算法）</h3><h4 id="算法："><a href="#算法：" class="headerlink" title="算法："></a>算法：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    nums = []</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersect</span><span class="params">(self, nums1: [int], nums2: [int])</span> -&gt; [int]:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums1:</span><br><span class="line">            <span class="keyword">if</span> nums2[<span class="number">0</span>] == i:</span><br><span class="line">                nums1.remove(i)</span><br><span class="line">                self.nums.append(i)</span><br><span class="line">        nums2.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> len(nums1) == <span class="number">0</span> <span class="keyword">or</span> len(nums2) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> self.nums</span><br><span class="line">        <span class="keyword">return</span> self.intersect(nums1,nums2)</span><br></pre></td></tr></table></figure>



<h3 id="方法二-排序比较"><a href="#方法二-排序比较" class="headerlink" title="方法二    排序比较"></a>方法二    排序比较</h3><h4 id="算法：-1"><a href="#算法：-1" class="headerlink" title="算法："></a>算法：</h4><blockquote>
<ul>
<li>对数组 nums1 和 nums2 排序。</li>
<li>初始化指针 i，j 和 k 为 0。</li>
<li>指针 i 指向 nums1，指针 j 指向 nums2：<ul>
<li>如果 nums1[i] &lt; nums2[j]，则 i++。</li>
<li>如果 nums1[i] &gt; nums2[j]，则 j++。</li>
<li>如果 nums1[i] == nums2[j]，将元素拷贝到 nums1[k]，且 i++，j++，k++。</li>
</ul>
</li>
<li>返回数组 nums1 前 k 个元素。</li>
</ul>
</blockquote>
<h4 id="图示："><a href="#图示：" class="headerlink" title="图示："></a>图示：</h4><p><img src="/2020/05/27/leetcode-1/2.png" alt="排序"></p>
<h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersect</span><span class="params">(self, nums1: [int], nums2: [int])</span> -&gt; [int]:</span></span><br><span class="line">        nums1.sort()</span><br><span class="line">        nums2.sort()</span><br><span class="line">        r = []</span><br><span class="line">        left, right = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; len(nums1) <span class="keyword">and</span> right &lt; len(nums2):</span><br><span class="line">            <span class="keyword">if</span> nums1[left] &lt; nums2[right]:</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> nums1[left] == nums2[right]:</span><br><span class="line">                r.append(nums1[left])</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">                right += <span class="number">1</span>    </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure>



<h3 id="方法三-哈希映射"><a href="#方法三-哈希映射" class="headerlink" title="方法三    哈希映射"></a>方法三    哈希映射</h3><h4 id="算法：-2"><a href="#算法：-2" class="headerlink" title="算法："></a>算法：</h4><blockquote>
<ul>
<li>如果 nums1 元素个数大于 nums2，则交换数组元素。</li>
<li>对于 nums1 的每个元素，添加到 HashMap m 中，如果元素已经存在则增加对应的计数。</li>
<li>初始化 k = 0，记录当前交集元素个数。</li>
<li>遍历数组 nums2：<ul>
<li>检查元素在 m 是否存在，若存在且计数为正：<ul>
<li>将元素拷贝到 nums1[k]，且 k++。</li>
<li>减少 m 中对应元素的计数。</li>
</ul>
</li>
</ul>
</li>
<li>返回 nums1 前 k 个元素。</li>
</ul>
</blockquote>
<h4 id="图示：-1"><a href="#图示：-1" class="headerlink" title="图示："></a>图示：</h4><p><img src="/2020/05/27/leetcode-1/1.png" alt="哈希映射"></p>
<h4 id="程序："><a href="#程序：" class="headerlink" title="程序："></a>程序：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersect</span><span class="params">(self, nums1: [int], nums2: [int])</span> -&gt; [int]:</span></span><br><span class="line">        n1,n2=collections.Counter(nums1),collections.Counter(nums2)</span><br><span class="line">        res=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> n1:</span><br><span class="line">            tmp=min(n1[i],n2[i])</span><br><span class="line">            <span class="keyword">while</span> tmp&gt;<span class="number">0</span>:</span><br><span class="line">                res.append(i)</span><br><span class="line">                tmp-=<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>



<h2 id="进阶问题"><a href="#进阶问题" class="headerlink" title="进阶问题"></a>进阶问题</h2><blockquote>
<p>问题：</p>
<ul>
<li>如果给定的数组已经排好序呢？你将如何优化你的算法？</li>
</ul>
<p>我的答案：</p>
<ul>
<li>不需要优化，如果排好序了这个算法效果更好。</li>
</ul>
</blockquote>
<blockquote>
<p>问题：</p>
<ul>
<li>如果 nums1 的大小比 nums2 小很多，哪种方法更优？</li>
</ul>
<p>我的答案：</p>
<ul>
<li>应该对算法进行优化，先比较一下两个数组的长度，每次循环比较小的数组。</li>
</ul>
</blockquote>
<blockquote>
<p>问题：</p>
<ul>
<li>如果 nums2 的元素存储在磁盘上，磁盘内存是有限的，并且你不能一次加载所有的元素到内存中，你该怎么办？</li>
</ul>
<p>我的答案：</p>
<ul>
<li>分批加载执行</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>算法</tag>
        <tag>哈希</tag>
        <tag>递归</tag>
        <tag>简单难度</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络笔记1 M-P模型</title>
    <url>/2020/05/27/mp/</url>
    <content><![CDATA[<h2 id="神经网络笔记1-–-M-P模型（神经元模型）"><a href="#神经网络笔记1-–-M-P模型（神经元模型）" class="headerlink" title="神经网络笔记1 – M-P模型（神经元模型）"></a>神经网络笔记1 – M-P模型（神经元模型）</h2><p>通过对生物神经元信息处理过程进行了简化和概括。M-P模型的神经元如图。</p>
<p><img src="/2020/05/27/mp/1.png" alt="图1"></p>
<h3 id="神经元的特性包括："><a href="#神经元的特性包括：" class="headerlink" title="神经元的特性包括："></a>神经元的特性包括：</h3><ol>
<li>多输入单输出</li>
<li>不同输入的权值不同</li>
<li>每个神经元都具有阈值</li>
<li>多个输入在处理体中进行累加，超过阈值输出1，小于阈值输出0。具体过程、公式表示如下图。</li>
</ol>
<p><img src="/2020/05/27/mp/2.png" alt="图2"></p>
<p><img src="/2020/05/27/mp/3.png" alt="图3"></p>
<h3 id="激活函数："><a href="#激活函数：" class="headerlink" title="激活函数："></a>激活函数：</h3><p>​        在处理体中判断累加和能否被激活的那个函数就是激活函数。上图中的激活函数就是sign()。具体概念如图。</p>
<p><img src="/2020/05/27/mp/4.png" alt="图4"></p>
<p>常见的激活函数如图：</p>
<p><img src="/2020/05/27/mp/5.png" alt="图5"></p>
]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>
