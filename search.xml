<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Tensorflow框架介绍</title>
    <url>/2020/06/09/tensorflow-1/</url>
    <content><![CDATA[<h3 id="Tensorflow框架介绍"><a href="#Tensorflow框架介绍" class="headerlink" title="Tensorflow框架介绍"></a>Tensorflow框架介绍</h3><h4 id="1-1-TF数据流图"><a href="#1-1-TF数据流图" class="headerlink" title="1.1 TF数据流图"></a>1.1 TF数据流图</h4><h5 id="Tensorflow结构分析"><a href="#Tensorflow结构分析" class="headerlink" title="Tensorflow结构分析"></a>Tensorflow结构分析</h5><blockquote>
<p>构建图阶段:建立流程图，包括定义数据（张量Tensor）和操作（节点Op）</p>
<p>执行图阶段：调用各方资源，将定义好的数据和操作运行起来</p>
</blockquote>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensorflow_dome</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#     原生python加法运算</span></span><br><span class="line">     a = <span class="number">2</span></span><br><span class="line">     b = <span class="number">3</span></span><br><span class="line">     c = a + b</span><br><span class="line">     print(<span class="string">"普通加法结果：\n"</span>,c)</span><br><span class="line">     </span><br><span class="line"><span class="comment">#     tensorflow 加法</span></span><br><span class="line">     a_t = tf.constant(<span class="number">2</span>)</span><br><span class="line">     b_t = tf.constant(<span class="number">3</span>)</span><br><span class="line">     c_t = a_t + b_t</span><br><span class="line">     print(<span class="string">"Tensorflow加法运算的结果：\n"</span>,c_t)</span><br><span class="line">     </span><br><span class="line"><span class="comment">#     开启会话</span></span><br><span class="line">     <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">         c_t_value = sess.run(c_t)</span><br><span class="line">         print(c_t_value)</span><br><span class="line">         </span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609153842522.png" alt="image-20200609153842522"></p>
<h4 id="1-2-图与TensorBoard"><a href="#1-2-图与TensorBoard" class="headerlink" title="1.2 图与TensorBoard"></a>1.2 图与TensorBoard</h4><h5 id="什么是图结构"><a href="#什么是图结构" class="headerlink" title="什么是图结构"></a>什么是图结构</h5><p>图包含了一组tf.Operation代表的计算单元对象和tf.Tensor代表的计算单元之间流动的数据。</p>
<h5 id="图相关操作"><a href="#图相关操作" class="headerlink" title="图相关操作"></a>图相关操作</h5><p> <strong>默认图</strong></p>
<p> 查看默认图的两种方法：</p>
<ul>
<li><p>通过调用tf.get_default_graph()访问，要将操作添加到默认图形中，直接创建OP即可。</p>
</li>
<li><p>op、sess都含有graph属性，默认都在一张图中。</p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">graph_demo</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#    图的演示</span></span><br><span class="line">     a_t = tf.constant(<span class="number">2</span>)</span><br><span class="line">     b_t = tf.constant(<span class="number">3</span>)</span><br><span class="line">     c_t = a_t + b_t</span><br><span class="line">     </span><br><span class="line"><span class="comment">#     查看默认图</span></span><br><span class="line"><span class="comment">#     方法一：调用方法</span></span><br><span class="line">     default_g = tf.get_default_graph()</span><br><span class="line">     print(<span class="string">"default_g:\n"</span>,default_g)</span><br><span class="line"><span class="comment">#     方法二：查看属性</span></span><br><span class="line">     print(<span class="string">"a_t的图属性"</span>,a_t.graph)</span><br><span class="line"></span><br><span class="line"><span class="comment">#     开启会话</span></span><br><span class="line">     <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">         c_t_value = sess.run(c_t)</span><br><span class="line">         print(<span class="string">"c_t_value："</span>,c_t_value)</span><br><span class="line">         print(<span class="string">"sess图属性："</span>,sess.graph)</span><br><span class="line">         </span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p> <strong>创建图</strong></p>
<p> 可以通过tf.Graph()自定义创建图</p>
<p> 举例：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">graph_demo2</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#    自定义图</span></span><br><span class="line">    new_g = tf.Graph()</span><br><span class="line"><span class="comment">#    在图中定义数据和操作</span></span><br><span class="line">    <span class="keyword">with</span> new_g.as_default():</span><br><span class="line">        a_new = tf.constant(<span class="number">20</span>)</span><br><span class="line">        b_new = tf.constant(<span class="number">30</span>)</span><br><span class="line">        c_new = a_new + b_new</span><br><span class="line">        print(<span class="string">"c_new:\n"</span>,c_new)</span><br><span class="line"> <span class="comment">#     开启会话</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session(graph=new_g) <span class="keyword">as</span> new_sess:</span><br><span class="line">        c_new_value = new_sess.run(c_new)</span><br><span class="line">        print(<span class="string">"c_t_value："</span>,c_new_value)</span><br><span class="line">        print(<span class="string">"sess图属性："</span>,new_sess.graph)</span><br></pre></td></tr></table></figure>



<h5 id="TensorBoard：可视化学习"><a href="#TensorBoard：可视化学习" class="headerlink" title="TensorBoard：可视化学习"></a>TensorBoard：可视化学习</h5><p> <strong>1. 数据序列化-events文件</strong></p>
<p> 运行代码将图序列化到本地，生成一个events文件</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.summary.FileWriter(path,graph&#x3D;sess.graph)</span><br></pre></td></tr></table></figure>

<p> <strong>2. 启动TensorBoard</strong></p>
<p> 运行命令生成图，再在浏览器输入127.0.0.1:6006查看。</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir&#x3D;&quot;.&#x2F;tmp&#x2F;summary&#x2F;&quot;</span><br></pre></td></tr></table></figure>



<h5 id="OP（operation）"><a href="#OP（operation）" class="headerlink" title="OP（operation）"></a>OP（operation）</h5><p>常见OP：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609181148718.png" alt="image-20200609181148718"></p>
<p>每一个图一个命名空间</p>
<h4 id="1-3-会话"><a href="#1-3-会话" class="headerlink" title="1.3 会话"></a>1.3 会话</h4><p>会话是一个运行Tensorflow operation的类，开启方式主要包括两种</p>
<ol>
<li><p>tf.Session:用于完整的程序中</p>
</li>
<li><p>tf.InteractiveSession：用于交互式上下文中，例如再cmd中打开运行以后就可以直接交互式的运行。</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609182000166.png" alt="image-20200609182000166"></p>
</li>
</ol>
<h5 id="创建会话初始化的参数、"><a href="#创建会话初始化的参数、" class="headerlink" title="创建会话初始化的参数、"></a>创建会话初始化的参数、</h5><ol>
<li><p>会话掌握资源，用完要回收。</p>
</li>
<li><p>初始化会话对象的参数：</p>
<ul>
<li>graph = None</li>
<li>target：如果设置为空，会话将使用本地计算机中的设备。还可以指定网址，一遍指定tensorflow服务器地址。</li>
<li>config：本参数允许指定一个tf.ConfigProto，以便控制会话的行为。</li>
</ul>
</li>
</ol>
<h5 id="会话的run"><a href="#会话的run" class="headerlink" title="会话的run()"></a>会话的run()</h5><ul>
<li>run(fetches,feed_dict=none,options=None,run_metadata=none)<ul>
<li>通过sess.run()运行operation</li>
<li>detches：单一的operation，或者列表、元祖</li>
<li>feed_dict：与tf.placeholder配合使用，运行时赋值使用<ul>
<li>placeholder提供占位符，run的时候通过feed_dict指定参数</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_demo</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#    图的演示</span></span><br><span class="line">     a_t = tf.placeholder(tf.float32)</span><br><span class="line">     b_t = tf.placeholder(tf.float32)</span><br><span class="line">     c_t = a_t + b_t</span><br><span class="line"></span><br><span class="line"><span class="comment">#     开启会话</span></span><br><span class="line">     <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">         c_t_value = sess.run(c_t,feed_dict=&#123;a_t:<span class="number">1</span>,b_t:<span class="number">2</span>&#125;)</span><br><span class="line">         print(<span class="string">"c_t_value："</span>,c_t_value)</span><br></pre></td></tr></table></figure>

<p>运行结果</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609203435131.png" alt="image-20200609203435131"></p>
<h4 id="1-4-张量"><a href="#1-4-张量" class="headerlink" title="1.4 张量"></a>1.4 张量</h4><h5 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h5><p>tensorflow就是一个n维数组，类型为tf.Tensor。</p>
<blockquote>
<p>张量在计算机中怎么存储？</p>
<p>标量  一个数字</p>
<p>数组  一维数组</p>
<p>矩阵  二维数组</p>
<p>张量  n维数组</p>
</blockquote>
<p><strong>张量的类型</strong></p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609211541820.png" alt="image-20200609211541820"></p>
<p><strong>张量的阶</strong></p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609211628549.png" alt="image-20200609211628549"></p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensor_demo</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#    张量的演示</span></span><br><span class="line">    tensor1 = tf.constant(<span class="number">4.0</span>)</span><br><span class="line">    tensor2 = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">    linear_square = tf.constant([[<span class="number">4</span>],[<span class="number">9</span>],[<span class="number">16</span>],[<span class="number">25</span>]],dtype=tf.int32)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"tensor1:"</span>,tensor1)</span><br><span class="line">    print(<span class="string">"tensor2:"</span>,tensor2)</span><br><span class="line">    print(<span class="string">"linear_square:"</span>,linear_square)</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609212425728.png" alt="image-20200609212425728"></p>
<p><strong>张量的变换</strong></p>
<ul>
<li>类型改变</li>
</ul>
<p><img src="/2020/06/09/tensorflow-1/image-20200609230835007.png" alt="image-20200609230835007"></p>
<blockquote>
<p>和ndarray属性修改对比：</p>
<ol>
<li><p>ndarray.astype(type)</p>
<p>tf.cast(tensor,dtype)</p>
<p>​    不会修改原始的tensor，返回新的改变类型后的tensor</p>
</li>
<li><p>ndarray.tostring</p>
</li>
</ol>
</blockquote>
<ul>
<li><p>形状改变</p>
<ul>
<li><p>静态形状：初始创建张量时的形状</p>
<p>如何改变静态形状？</p>
<blockquote>
<p>什么情况下才可以改变/更新静态形状？</p>
<p>在形状还没有完全固定下来额时候 </p>
</blockquote>
<p>tensor.set_shape()</p>
<p>只能更新形状没确定部分。</p>
</li>
<li><p>动态形状</p>
<ul>
<li>可能随意改变形状，但是数据总量不能改变。</li>
<li>tf.reshape(tensor,shape )<ul>
<li>不会改变原始的tensor</li>
<li>返回新的改变形状后的tensor</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a_t = tf.placeholder(tf.float32,shape=[<span class="literal">None</span>,<span class="literal">None</span>])</span><br><span class="line">    b_t = tf.placeholder(tf.float32,shape=[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line"><span class="comment">#    形状确定的</span></span><br><span class="line">    c_t = tf.placeholder(tf.float32,shape=[<span class="number">3</span>,<span class="number">2</span>])</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"a_t:"</span>,a_t)</span><br><span class="line">    print(<span class="string">"b_t:"</span>,b_t)</span><br><span class="line">    print(<span class="string">"c_t:"</span>,c_t)</span><br><span class="line"><span class="comment">#   修改静态形状</span></span><br><span class="line">    b_t.set_shape([<span class="number">2</span>,<span class="number">10</span>])</span><br><span class="line"><span class="comment">#    修改动态形状</span></span><br><span class="line">    tf.reshape(c_t,shape=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line">    print(<span class="string">"b_t:"</span>,b_t)</span><br><span class="line">    print(<span class="string">"c_t:"</span>,c_t)</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200609235623814.png" alt="image-20200609235623814"></p>
<h4 id="1-5-API"><a href="#1-5-API" class="headerlink" title="1.5 API"></a>1.5 API</h4><h5 id="基础API"><a href="#基础API" class="headerlink" title="基础API"></a>基础API</h5><ul>
<li><strong>tf.app</strong>:相当于为tensorflow提供的一个main函数</li>
<li><strong>tf.image</strong>:图像处理的操作。</li>
<li><strong>tf.gfile</strong>:文件操作函数。</li>
<li><strong>tf.summary</strong>:用来生成TensorBoard可用的统计日志</li>
<li><strong>tf.python_io</strong>:数据读取</li>
<li><strong>tf.train</strong>：提供一些训练器</li>
<li><strong>tf.nn</strong>:一些构建神经网络的底层函数</li>
</ul>
<h5 id="高级API"><a href="#高级API" class="headerlink" title="高级API"></a>高级API</h5><p><strong>tf.keras</strong>:本来是一个独立的深度学习库，tf用来快速构建模型。</p>
<p><strong>tf.layers</strong>:以更高级的概念定义一个模型</p>
<p><strong>tf.contrib</strong>：构建计算图的高级操作。</p>
<p><strong>tf.estimator</strong>:相当于构建模型、训练、评价的合体</p>
<h5 id="api层次划分"><a href="#api层次划分" class="headerlink" title="api层次划分"></a>api层次划分</h5><p><img src="/2020/06/09/tensorflow-1/image-20200610072817901.png" alt="image-20200610072817901"></p>
<h4 id="1-6-案例：实现线性回归的训练"><a href="#1-6-案例：实现线性回归的训练" class="headerlink" title="1.6 案例：实现线性回归的训练"></a>1.6 案例：实现线性回归的训练</h4><p>一次函数的线性回归，随机指定一百个点</p>
<p>代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_regression</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#    实现一个线性回归</span></span><br><span class="line"><span class="comment">#    1. 准备数据</span></span><br><span class="line">    X = tf.random_normal(shape=[<span class="number">100</span>,<span class="number">1</span>])</span><br><span class="line">    y_true = tf.matmul(X,[[<span class="number">0.8</span>]]) + <span class="number">0.6</span></span><br><span class="line"><span class="comment">#    2.构建模型</span></span><br><span class="line">    weight = tf.Variable(initial_value=tf.random_normal(shape=[<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line">    bias = tf.Variable(initial_value=tf.random_normal(shape=[<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line">    y_predict = tf.matmul(X,weight) + bias</span><br><span class="line"><span class="comment">#    3.构建损失函数</span></span><br><span class="line">    error = tf.reduce_mean(tf.square(y_predict - y_true))</span><br><span class="line"><span class="comment">#    4.优化损失</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(error)</span><br><span class="line">    </span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init)</span><br><span class="line">        print(<span class="string">"训练前模型参数为：权重%f 偏置%f 损失%f"</span>%(weight.eval(),bias.eval(),error.eval()))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">            sess.run(optimizer)</span><br><span class="line">        print(<span class="string">"训练后模型参数为：权重%f 偏置%f 损失%f"</span>%(weight.eval(),bias.eval(),error.eval()))</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610102402102.png" alt="image-20200610102402102"></p>
<h4 id="1-7-需要注意的问题"><a href="#1-7-需要注意的问题" class="headerlink" title="1.7 需要注意的问题"></a>1.7 需要注意的问题</h4><h5 id="学习率的设置、步数的设置与梯度爆炸"><a href="#学习率的设置、步数的设置与梯度爆炸" class="headerlink" title="学习率的设置、步数的设置与梯度爆炸"></a>学习率的设置、步数的设置与梯度爆炸</h5><p>学习率越大，训练到较好结果的步数越小；学习率越小，训练到较好结果的步数越大。</p>
<p>学习率过大会导致梯度爆炸的情况。</p>
<blockquote>
<p>在极端情况下，权重的值变得非常大，以至于溢出导致NaN.</p>
<p>如何解决梯度爆炸问题？</p>
<ol>
<li>重新设计网络</li>
<li>调整学习率</li>
<li>使用梯度阶段（在训练过程中检查和限制梯度的大小）</li>
<li>使用激活函数</li>
</ol>
</blockquote>
<p><strong>trainable属性</strong></p>
<p>在定义变量时属性可以规定本trainable数据是否可以训练。</p>
<h4 id="1-8-案例改进"><a href="#1-8-案例改进" class="headerlink" title="1.8 案例改进"></a>1.8 案例改进</h4><p>对案例在三方面进行了改进，分别是增加变量显示，增加命名空间、模型的保存与加载。</p>
<ul>
<li><p>增加变量显示</p>
<ol>
<li>创建事件文件</li>
<li>收集变量</li>
<li>合并变量</li>
<li>每次迭代运行一次合并变量</li>
<li>每次迭代将summary对象写入事件文件</li>
</ol>
</li>
<li><p>增加命名空间</p>
<ul>
<li>为每个部分增加命名空间可以在tensorboard里更清楚地观察模型</li>
</ul>
</li>
<li><p>模型的保存与加载</p>
<ol>
<li><p>实例化Saver</p>
</li>
<li><p>保存（在训练迭代过程中对会话进行保存）</p>
<p>saver.save(sess,path)</p>
</li>
<li><p>加载（对会话进行加载）</p>
<p>先判断模型是否存在，存在再进行加载。</p>
</li>
</ol>
</li>
</ul>
<p>代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_regression</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#    实现一个线性回归</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"prepare_data"</span>):</span><br><span class="line">    <span class="comment">#    1. 准备数据</span></span><br><span class="line">        X = tf.random_normal(shape=[<span class="number">100</span>,<span class="number">1</span>],name=<span class="string">"feature"</span>)</span><br><span class="line">        y_true = tf.matmul(X,[[<span class="number">0.8</span>]]) + <span class="number">0.6</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"create_model"</span>):</span><br><span class="line">    <span class="comment">#    2.构建模型</span></span><br><span class="line">        weight = tf.Variable(initial_value=tf.random_normal(shape=[<span class="number">1</span>,<span class="number">1</span>]),name=<span class="string">"Weights"</span>)</span><br><span class="line">        bias = tf.Variable(initial_value=tf.random_normal(shape=[<span class="number">1</span>,<span class="number">1</span>]),name=<span class="string">"bias"</span>)</span><br><span class="line">        y_predict = tf.matmul(X,weight) + bias</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"loss_function"</span>):</span><br><span class="line">    <span class="comment">#    3.构建损失函数</span></span><br><span class="line">        error = tf.reduce_mean(tf.square(y_predict - y_true))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"optimizer"</span>):</span><br><span class="line">    <span class="comment">#    4.优化损失</span></span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(error)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#    2_收集变量</span></span><br><span class="line"><span class="comment">#    scalar用来收集</span></span><br><span class="line">    tf.summary.scalar(<span class="string">"error"</span>,error)</span><br><span class="line">    tf.summary.histogram(<span class="string">"weights"</span>,weight)</span><br><span class="line">    tf.summary.histogram(<span class="string">"bias"</span>,bias)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#    3_合并变量</span></span><br><span class="line">    merged = tf.summary.merge_all()</span><br><span class="line"> </span><br><span class="line"><span class="comment">#    创建实例化Saver</span></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#    初始化所有数据</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#    开始会话</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"><span class="comment">#        初始化变量</span></span><br><span class="line">        sess.run(init)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#        1_创建事件文件</span></span><br><span class="line">        file_writer = tf.summary.FileWriter(<span class="string">"./tmp/linear"</span>,graph=sess.graph)</span><br><span class="line">        </span><br><span class="line">        print(<span class="string">"训练前模型参数为：权重%f 偏置%f 损失%f"</span>%(weight.eval(),bias.eval(),error.eval()))</span><br><span class="line"><span class="comment">#        开始训练</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            sess.run(optimizer)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 运行合并操作</span></span><br><span class="line">            summary = sess.run(merged)</span><br><span class="line">            <span class="comment"># 将每次迭代后的</span></span><br><span class="line">            file_writer.add_summary(summary,i)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 保存模型</span></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                saver.save(sess,<span class="string">"./tmp/model/my_linear.ckpt"</span>)</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        print(<span class="string">"训练后模型参数为：权重%f 偏置%f 损失%f"</span>%(weight.eval(),bias.eval(),error.eval()))</span><br></pre></td></tr></table></figure>

<p>增加命名空间后生成的tensorboard图：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610171639402.png" alt="image-20200610171639402"></p>
<p>模型保存后生成的文件如图，里面没有具体的ckpt文件</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610171914326.png" alt="image-20200610171914326"></p>
<p>加载模型只需把原有训练模型的位置改为下面的代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> os.path.exists(<span class="string">"./tmp/model/checkpoint"</span>):</span><br><span class="line">    saver.restore(sess,<span class="string">"./tmp/model/my_linear.ckpt"</span>)</span><br></pre></td></tr></table></figure>

<p>加载后运行结果：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610172424010.png" alt="image-20200610172424010"></p>
<h4 id="1-9-命令行参数使用"><a href="#1-9-命令行参数使用" class="headerlink" title="1.9 命令行参数使用"></a>1.9 命令行参数使用</h4><ol>
<li><p>通过tf.app.flags定义从命令行接受的参数</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610174302834.png" alt="image-20200610174302834"></p>
</li>
<li><p>简化变量名</p>
</li>
</ol>
<p>例子：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610174432963.png" alt="image-20200610174432963"></p>
<p>运行：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610174450754.png" alt="image-20200610174450754"></p>
<p>命令行运行：</p>
<p><img src="/2020/06/09/tensorflow-1/image-20200610174536346.png" alt="image-20200610174536346"></p>
<blockquote>
<p><strong>tf.app.run函数</strong></p>
<p>tf.app.run()会自动调用程序中的main(argv)函数</p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>学习笔记</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>基于矩阵操作的必然性QoS约束副本放置方法 笔记</title>
    <url>/2020/06/06/paperNote1/</url>
    <content><![CDATA[<blockquote>
<p>论文名称：基于矩阵操作的必然性QoS约束副本放置方法</p>
<p>作者：付伟，叶清，吴晓平</p>
<p>发表时间：2012.12</p>
<p>发表期刊：系统工程理论实践</p>
</blockquote>
<h2 id="阅读笔记"><a href="#阅读笔记" class="headerlink" title="阅读笔记"></a>阅读笔记</h2><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h3><p>这篇论文讲的就是用矩阵计算的方法在满足必然性约束的条件下怎么在分布式系统中进行副本放置。</p>
<h4 id="1-1-副本放置问题"><a href="#1-1-副本放置问题" class="headerlink" title="1.1 副本放置问题"></a>1.1 副本放置问题</h4><h5 id="副本技术"><a href="#副本技术" class="headerlink" title="副本技术"></a>副本技术</h5><blockquote>
<p>副本技术是应用在分布式系统中，把服务器中的数据通过在其他机器中建立副本的方式，用来提升系统的整体性能。副本技术通过牺牲一定的存储开销和一致性维护开销，方便用户以“就近原则”访问共享数据资源，从而使系统性能全面提升。</p>
</blockquote>
<h5 id="副本放置问题"><a href="#副本放置问题" class="headerlink" title="副本放置问题"></a>副本放置问题</h5><blockquote>
<p>副本放置问题是副本技术要解决的核心问题，决定副本系统中需要多少个副本和这些副本如何分布，从而能够达到某种性能指标或者系统设计要求。</p>
</blockquote>
<h4 id="1-2-必然性QoS约束"><a href="#1-2-必然性QoS约束" class="headerlink" title="1.2 必然性QoS约束"></a>1.2 必然性QoS约束</h4><h5 id="QoS-服务质量"><a href="#QoS-服务质量" class="headerlink" title="QoS(服务质量)"></a>QoS(服务质量)</h5><blockquote>
<p>服务质量描述一个服务满足客户需求的能力，是服务好坏的定量度量。</p>
</blockquote>
<h5 id="或然性服务质量约束（总体服务质量约束）"><a href="#或然性服务质量约束（总体服务质量约束）" class="headerlink" title="或然性服务质量约束（总体服务质量约束）"></a>或然性服务质量约束（总体服务质量约束）</h5><blockquote>
<p>从系统的全局出发，将服务的总体指标作为QoS评价的依据。以全局服务质量指标作为优化目标。</p>
</blockquote>
<h5 id="必然性服务质量约束（个体服务质量约束）"><a href="#必然性服务质量约束（个体服务质量约束）" class="headerlink" title="必然性服务质量约束（个体服务质量约束）"></a>必然性服务质量约束（个体服务质量约束）</h5><blockquote>
<p>每个用户的QoS请求都100%的得到满足。比如在股票市场上, 股东需要实时获得股票信息当股票信息发生变化时, 每个股东都必须在第一时间内获取到最新的股票价格 如果因为股票信息系统的原因导致某个股东在经过一段时间延迟之后才获得所需数据, 这将使其错过股票交易的最佳时机, 从而可能蒙受重大经济损失。</p>
</blockquote>
<h5 id="两种服务质量约束举例说明"><a href="#两种服务质量约束举例说明" class="headerlink" title="两种服务质量约束举例说明"></a>两种服务质量约束举例说明</h5><p>总体服务质量约束和个体服务质量约束都是对服务质量的要求。如图片所示是三种客户节点访问服务器的情况。第一个是未设置副本节点的情况下客户节点访问服务器，此时系统的总消耗是45，平均每个节点的消耗是4.5。第二个是在满足总体服务质量约束的条件下客户节点访问服务器节点和副本节点，此时系统的总消耗是13，系统的平均消耗是1.3，此时是放置一个副本情况下平均开销最小的情况，也就是满足总体服务质量约束最好的情况。第三个是满足所有客户节点访问服务的消耗都不大于3的个体服务质量约束的条件下客户节点的消耗，此时系统的总开销15，平均开销1.5，单个节点消耗的最大值是3，满足个体服务质量约束。第二个虽然整体的平均开销情况最好，但是不满足个体服务质量约束。</p>
<p><img src="/2020/06/06/paperNote1/image-20200606215837566.png" alt="image-20200606215837566"></p>
<h5 id="必然性约束的表示"><a href="#必然性约束的表示" class="headerlink" title="必然性约束的表示"></a>必然性约束的表示</h5><blockquote>
<p> 所用到的符号的说明</p>
<p><img src="/2020/06/06/paperNote1/image-20200606230820213.png" alt="image-20200606230820213"></p>
<p><strong>QoS属性满足</strong></p>
<p><img src="/2020/06/06/paperNote1/image-20200606231118247.png" alt="image-20200606231118247"></p>
<p><strong>QoS请求差额</strong></p>
<p><img src="/2020/06/06/paperNote1/C:%5CHexo%5Csource_posts%5CpaperNote1%5Cimage-20200606231303225.png" alt="image-20200606231303225"></p>
<p><strong>必然性约束</strong></p>
<p><img src="/2020/06/06/paperNote1/image-20200606231919366.png" alt="image-20200606231919366"></p>
<p><strong>必然性约束满足</strong></p>
<p>​        如果必然性约束QoS_Certain(S) = 0，则称服务S是必然性约束满足的。</p>
</blockquote>
<h4 id="1-3-矩阵操作"><a href="#1-3-矩阵操作" class="headerlink" title="1.3 矩阵操作"></a>1.3 矩阵操作</h4><h5 id="矩阵最小并"><a href="#矩阵最小并" class="headerlink" title="矩阵最小并"></a>矩阵最小并</h5><p>对于两个形状相同的矩阵X、Y，两个矩阵的矩阵最小并记作Z=X△Y，Z等于X和Y中对应每个位置元素的较小值。</p>
<h5 id="矩阵正相差"><a href="#矩阵正相差" class="headerlink" title="矩阵正相差"></a>矩阵正相差</h5><p>对于两个形状相同的矩阵X、Y，两个矩阵的矩阵正相差记作Z=X▽Y，其中Z中每个位置元素的值如图所示。</p>
<p><img src="/2020/06/06/paperNote1/image-20200606233216842.png" alt="image-20200606233216842"></p>
<h5 id="矩阵和"><a href="#矩阵和" class="headerlink" title="矩阵和"></a>矩阵和</h5><p>矩阵和就是矩阵中所有元素的代数和。</p>
<p><img src="/2020/06/06/paperNote1/image-20200606233342102.png" alt="image-20200606233342102"></p>
<h4 id="1-4-矩阵表示"><a href="#1-4-矩阵表示" class="headerlink" title="1.4 矩阵表示"></a>1.4 矩阵表示</h4><p>假设系统中包含n个节点，每个节点的QOS约束包括m个元素。</p>
<h5 id="质量约束矩阵-BM"><a href="#质量约束矩阵-BM" class="headerlink" title="质量约束矩阵(BM)"></a>质量约束矩阵(BM)</h5><p>质量约束矩阵中存储每个节点的每个属性的QoS需求。</p>
<p><img src="/2020/06/06/paperNote1/image-20200606234233744.png" alt="image-20200606234233744"></p>
<h5 id="服务质量矩阵（QMj）"><a href="#服务质量矩阵（QMj）" class="headerlink" title="服务质量矩阵（QMj）"></a>服务质量矩阵（QMj）</h5><p>服务质量矩阵表示当j节点单独提供服务时每个节点所获得的不同属性的QoS质量。</p>
<p><img src="/2020/06/06/paperNote1/paperNote1%5Cimage-20200606234452406.png" alt="image-20200606234452406"></p>
<h5 id="加权向量（WV）"><a href="#加权向量（WV）" class="headerlink" title="加权向量（WV）"></a>加权向量（WV）</h5><p>加权向量是指每个属性在QoS约束中所占的比重。</p>
<p><img src="/2020/06/06/paperNote1/image-20200606234618642.png" alt="image-20200606234618642"></p>
<h5 id="服务质量矩阵（DM）"><a href="#服务质量矩阵（DM）" class="headerlink" title="服务质量矩阵（DM）"></a>服务质量矩阵（DM）</h5>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>副本放置</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯网络3 贝叶斯网络</title>
    <url>/2020/06/04/bayes-3/</url>
    <content><![CDATA[<h2 id="贝叶斯计算"><a href="#贝叶斯计算" class="headerlink" title="贝叶斯计算"></a>贝叶斯计算</h2><h3 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h3><h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><h5 id="独立"><a href="#独立" class="headerlink" title="独立"></a>独立</h5><blockquote>
<p>事件X和Y相互独立，则</p>
<ul>
<li><p>P(X,Y) = P(X)P(Y)</p>
</li>
<li><p>P(X|Y) = P(X)</p>
</li>
</ul>
</blockquote>
<h5 id="条件独立"><a href="#条件独立" class="headerlink" title="条件独立"></a>条件独立</h5><blockquote>
<p>如果在给定Z的条件下，X和Y相互独立</p>
<ul>
<li>P(X|Y,Z) = P(X|Z)</li>
</ul>
<p>就是Z条件下X的概率不受Y的影响</p>
</blockquote>
<h5 id="联合概率"><a href="#联合概率" class="headerlink" title="联合概率"></a>联合概率</h5><blockquote>
<p>P(X1,X2…Xn)表示X1,X2…Xn同时发生的概率</p>
<p>如果X1,X2…Xn相互独立：</p>
<ul>
<li><em>P</em>(<em>X</em>1, <em>X</em>2, …, Xn) = <em>P</em>(<em>X</em>1) <em>P</em>(<em>X</em>2) …<em>P</em>(Xn)</li>
</ul>
<p>条件概率：</p>
<ul>
<li><em>P</em>(<em>X</em>1, <em>X</em>2, …, Xn) = <em>P</em>(<em>X</em>1|<em>X</em>2, …,Xn) <em>P</em>(<em>X</em>2, …, Xn)</li>
</ul>
<p>迭代表示：</p>
<ul>
<li>​     <em>P</em>(<em>X</em>1, <em>X</em>2, …, Xn)</li>
<li>​            = <em>P</em>(<em>X</em>1) <em>P</em>(<em>X</em>2| <em>X</em>1) <em>P</em>(<em>X</em>3| <em>X</em>2<em>X</em>1)…<em>P</em>(Xn|Xn-1, …, X1)</li>
<li>​            = <em>P</em>(<em>X**N</em>) <em>P</em>(<em>X**N</em>-1| <em>X**N</em>) <em>P</em>(<em>X**N</em>-2| <em>X**N</em>-1<em>X**N</em>)…<em>P</em>(<em>X</em>1|<em>X</em>2, …, <em>X**N</em>)</li>
</ul>
</blockquote>
<h5 id="贝叶斯公式："><a href="#贝叶斯公式：" class="headerlink" title="贝叶斯公式："></a>贝叶斯公式：</h5><h4 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h4><p><img src="/2020/06/04/bayes-3/image-20200604093939830.png" alt="image-20200604093939830"></p>
<h3 id="主观贝叶斯"><a href="#主观贝叶斯" class="headerlink" title="主观贝叶斯"></a>主观贝叶斯</h3><h4 id="公式-1"><a href="#公式-1" class="headerlink" title="公式"></a>公式</h4><h5 id="规则的不确定性"><a href="#规则的不确定性" class="headerlink" title="规则的不确定性"></a>规则的不确定性</h5><blockquote>
<p>LS：表示A为真时对B的影响。</p>
<p><img src="/2020/06/04/bayes-3/image-20200604094349083.png" alt="image-20200604094349083"></p>
<p>LN：表示A为假时对B的影响。(LN表示的就是规则的不确定性，贝叶斯网络中只考虑了规则的确定性，没考虑当A为假的情况)</p>
<p><img src="/2020/06/04/bayes-3/image-20200604094422795.png" alt="image-20200604094422795"></p>
</blockquote>
<h4 id="例题-1"><a href="#例题-1" class="headerlink" title="例题"></a>例题</h4>]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>贝叶斯网络</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯网络2 贝叶斯公式</title>
    <url>/2020/05/31/bayes-2/</url>
    <content><![CDATA[<h2 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h2>]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>贝叶斯网络</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯网络1 概率基础</title>
    <url>/2020/05/31/nayes-1/</url>
    <content><![CDATA[<h2 id="概率基础"><a href="#概率基础" class="headerlink" title="概率基础"></a>概率基础</h2><h3 id="随机试验和随机事件"><a href="#随机试验和随机事件" class="headerlink" title="随机试验和随机事件"></a>随机试验和随机事件</h3><p>所涉及的基本概念：</p>
<blockquote>
<p>试验：为了查看某件事的结果或某物的性能而从事的某种活动。</p>
<p>随机试验（简称试验）：满足如下几个条件的试验就是随机试验：</p>
<blockquote>
<p>可重复性：在相同条件下可重复进行</p>
<p>不确定性：每次试验前不能准确预测哪一个结果会发生</p>
<p>可观察性：每次试验结果可能不止一个，并且事先知道所有可能结果</p>
</blockquote>
<p>基本事件（样本点）：每一种可能出现的情况</p>
<p>样本空间：所有样本点的集合</p>
<p>随机事件（简称事件）：由基本事件复合而成的事件</p>
<p>必然事件：一定会发生的事件</p>
<p>不可能事件：一定不会发生的事件</p>
<p>完备事件组：一组事件且两两没有交集，所有事件并起来就是样本空间</p>
</blockquote>
<p>用韦恩图进行表示：</p>
<p><img src="/2020/05/31/nayes-1/image-20200531151801359.png" alt="image-20200531151801359"></p>
<h3 id="概率及其性质"><a href="#概率及其性质" class="headerlink" title="概率及其性质"></a>概率及其性质</h3><ul>
<li><p>概率用来描述随机事件发生的可能性的大小，值在0到1之间。</p>
</li>
<li><p>性质：</p>
<p><img src="/2020/05/31/nayes-1/image-20200531160806048.png" alt="image-20200531160806048"></p>
</li>
</ul>
<h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><p><img src="/2020/05/31/nayes-1/image-20200531160938552.png" alt="image-20200531160938552"></p>
<h4 id="事件独立"><a href="#事件独立" class="headerlink" title="事件独立"></a>事件独立</h4><blockquote>
<p> <strong>定义：</strong></p>
<p>如果两个事件事件是否发生互相没有影响，称两个事件互相独立</p>
<p>设A、B两个事件，B发生的可能性不受到A的影响，即P(B|A)=P(B)，则A B两个事件相互独立</p>
</blockquote>
<blockquote>
<p><strong>充要条件：</strong></p>
<p>P(AB) = P(A)P(B)</p>
</blockquote>
<blockquote>
<p><strong>互不相容和互相独立</strong>：</p>
<ul>
<li><p>互不相容是两个事件没有交集</p>
</li>
<li><p>相互独立是两个事件没有影响</p>
</li>
<li><p>没有交集的两个事件也可能是有影响的</p>
</li>
</ul>
</blockquote>
<h3 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h3><p>全概率定理：如果事件A1、A2 … An是一个完备事件组，并且都有正概率，则有：</p>
<p>P(B) = P(A_{1})P(B|A_{1}) + P(A_{2})P(B|A_{2}) + … + P(A_{n})P(B|A_{n})</p>
<p> 一个复杂的概率事件问题可以转化为不同情况或者不同原因下发生的简单事件的概率求和问题。</p>
<p><img src="/2020/05/31/nayes-1/image-20200531164456756.png" alt="image-20200531164456756"></p>
<p>举例：</p>
<p><img src="/2020/05/31/nayes-1/image-20200531164542739.png" alt="image-20200531164542739"></p>
]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>贝叶斯网络</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode-3</title>
    <url>/2020/05/29/leetcode-3/</url>
    <content><![CDATA[<h2 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h2><blockquote>
<p>题目编号：198</p>
<p>题目标题：打家劫舍</p>
<p>题目难度： 简单</p>
<p>说明：</p>
<p>你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。</p>
<p>给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。</p>
<p>示例1：</p>
<blockquote>
<p>输入：[1,2,3,1]</p>
<p>输出：4</p>
</blockquote>
<p>示例2：</p>
<blockquote>
<p>输入：[2,7,9,3,1]</p>
<p>输出：12</p>
</blockquote>
</blockquote>
<h2 id="题目解答"><a href="#题目解答" class="headerlink" title="题目解答"></a>题目解答</h2><h3 id="方法-动态规划"><a href="#方法-动态规划" class="headerlink" title="方法    动态规划"></a>方法    动态规划</h3><p>思路：</p>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>简单难度</tag>
        <tag>动态规划</tag>
        <tag>滚动数组</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络笔记2 神经网络模型</title>
    <url>/2020/05/28/networkModel/</url>
    <content><![CDATA[<h2 id="神经网络笔记2-–-神经网络模型"><a href="#神经网络笔记2-–-神经网络模型" class="headerlink" title="神经网络笔记2 – 神经网络模型"></a>神经网络笔记2 – 神经网络模型</h2><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>神经网络的具体分类如图：</p>
<p><img src="/2020/05/28/networkModel/1.png" alt="分类"></p>
<p>各模型结构：</p>
<blockquote>
<p>层次模型中单纯层次结构、层内有互联、输出层到输入层有连接结构如图：</p>
<p><img src="/2020/05/28/networkModel/2.png" alt="层次模型"></p>
<p>互连模型中全互连和局部互连模型如图：</p>
<p><img src="/2020/05/28/networkModel/3.png" alt="互联模型"></p>
<p>前馈性网络和反馈性网络的结构如图：</p>
<p><img src="/2020/05/28/networkModel/4.png" alt="前馈网络和反馈网络"></p>
</blockquote>
<h3 id="前馈神经网络和反馈神经网络"><a href="#前馈神经网络和反馈神经网络" class="headerlink" title="前馈神经网络和反馈神经网络"></a>前馈神经网络和反馈神经网络</h3><p><strong>前馈神经网络：</strong></p>
<blockquote>
<p>前馈神经网络是一种最简单的神经网络，采用单向多层结构。</p>
<p><img src="/2020/05/28/networkModel/5.png" alt="前馈神经网络结构"></p>
</blockquote>
<p>反馈神经网络：</p>
<blockquote>
<p>反馈神经网络将输出再返回到输入进行训练</p>
<p>常见的反馈神经网络：Hopfield神经网络、Elman神经网络、Boltzmann</p>
</blockquote>
<p><strong>前馈神经网络与反馈神经网络的结构</strong>：</p>
<blockquote>
<p><img src="/2020/05/28/networkModel/6.png" alt="结构"></p>
</blockquote>
<p><strong>前馈神经网络与反馈神经网络的结构</strong></p>
<blockquote>
<p><img src="/2020/05/28/networkModel/7.png" alt="区别"></p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode-2</title>
    <url>/2020/05/28/leetcode-2/</url>
    <content><![CDATA[<h2 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h2><blockquote>
<p>题目编号：136</p>
<p>题目标题：只出现一次的数字</p>
<p>题目难度： 简单</p>
<p>说明：</p>
<p>给定一个<strong>非空</strong>整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。</p>
<ul>
<li>算法具有线性时间复杂度。</li>
<li>不使用额外的空间。</li>
</ul>
<p>示例1：</p>
<blockquote>
<p>输入：[2,2,1]</p>
<p>输出：1</p>
</blockquote>
<p>示例2：</p>
<blockquote>
<p>输入：[4,1,2,1,2]</p>
<p>输出：4</p>
</blockquote>
</blockquote>
<h2 id="题目解答"><a href="#题目解答" class="headerlink" title="题目解答"></a>题目解答</h2><h3 id="方法一-异或"><a href="#方法一-异或" class="headerlink" title="方法一     异或"></a>方法一     异或</h3><h4 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h4><blockquote>
<ul>
<li>两个相同数字的异或结果为0，对所有数字进行异或运算，所有相同的数字都相互抵消，最后剩下的就是出现一次的数字。</li>
<li>因为异或运算满足交换率和结合率，所以数字顺序不影响运算。</li>
<li><img src="/2020/05/28/leetcode-2/0.png" alt="交换"></li>
</ul>
</blockquote>
<h4 id="图示-："><a href="#图示-：" class="headerlink" title="图示 ："></a>图示 ：</h4><p><img src="/2020/05/28/leetcode-2/1.png" alt="异或"></p>
<h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">singleNumber</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">        a = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            a ^= i</span><br><span class="line">        <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure>



<h3 id="方法二-数学计算"><a href="#方法二-数学计算" class="headerlink" title="方法二    数学计算"></a>方法二    数学计算</h3><h4 id="思路：-1"><a href="#思路：-1" class="headerlink" title="思路："></a>思路：</h4><blockquote>
<p>先通过set把数据去重，然后把所有的值相加*2去减之前的值，剩下的值就是答案</p>
</blockquote>
<h4 id="代码：-1"><a href="#代码：-1" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">singleNumber</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span> * sum(set(nums)) - sum(nums)</span><br></pre></td></tr></table></figure>



<h3 id="方法三-Counter"><a href="#方法三-Counter" class="headerlink" title="方法三     Counter"></a>方法三     Counter</h3><h4 id="代码：-2"><a href="#代码：-2" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">singleNumber</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        datas = Counter(nums)</span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> datas:</span><br><span class="line">            <span class="keyword">if</span> datas[each] == <span class="number">1</span>: <span class="keyword">return</span> each</span><br></pre></td></tr></table></figure>

<h3 id="方法四-数组切片（费时）"><a href="#方法四-数组切片（费时）" class="headerlink" title="方法四     数组切片（费时）"></a>方法四     数组切片（费时）</h3><h4 id="代码：-3"><a href="#代码：-3" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def singleNumber(self, nums: List[int]) -&gt; int:</span><br><span class="line">        for i in range(len(nums)):</span><br><span class="line">            if nums[i] not in nums[0:i] and nums[i] not in nums[i+1:]: return nums[i]</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>位运算</tag>
        <tag>简单难度</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode-1</title>
    <url>/2020/05/27/leetcode-1/</url>
    <content><![CDATA[<h2 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h2><blockquote>
<p>题目编号：350</p>
<p>题目标题：两个数组的交集 Ⅱ</p>
<p>说明：</p>
<ul>
<li>输出结果中每个元素出现的次数，应与元素在两个数组中出现的次数一致。</li>
<li>我们可以不考虑输出结果的顺序。</li>
</ul>
<p>示例1：</p>
<blockquote>
<p>输入：nums1 = [1,2,2,1], nums2 = [2,2]</p>
<p>输出：[2,2]</p>
</blockquote>
<p>示例2：</p>
<blockquote>
<p>输入：nums1 = [4,9,5], nums2 = [9,4,9,8,4]</p>
<p>输出：[4,9]</p>
</blockquote>
</blockquote>
<h2 id="题目解答"><a href="#题目解答" class="headerlink" title="题目解答"></a>题目解答</h2><h3 id="方法一-递归调用（自己写的算法）"><a href="#方法一-递归调用（自己写的算法）" class="headerlink" title="方法一     递归调用（自己写的算法）"></a>方法一     递归调用（自己写的算法）</h3><h4 id="算法："><a href="#算法：" class="headerlink" title="算法："></a>算法：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    nums = []</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersect</span><span class="params">(self, nums1: [int], nums2: [int])</span> -&gt; [int]:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums1:</span><br><span class="line">            <span class="keyword">if</span> nums2[<span class="number">0</span>] == i:</span><br><span class="line">                nums1.remove(i)</span><br><span class="line">                self.nums.append(i)</span><br><span class="line">        nums2.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> len(nums1) == <span class="number">0</span> <span class="keyword">or</span> len(nums2) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> self.nums</span><br><span class="line">        <span class="keyword">return</span> self.intersect(nums1,nums2)</span><br></pre></td></tr></table></figure>



<h3 id="方法二-排序比较"><a href="#方法二-排序比较" class="headerlink" title="方法二    排序比较"></a>方法二    排序比较</h3><h4 id="算法：-1"><a href="#算法：-1" class="headerlink" title="算法："></a>算法：</h4><blockquote>
<ul>
<li>对数组 nums1 和 nums2 排序。</li>
<li>初始化指针 i，j 和 k 为 0。</li>
<li>指针 i 指向 nums1，指针 j 指向 nums2：<ul>
<li>如果 nums1[i] &lt; nums2[j]，则 i++。</li>
<li>如果 nums1[i] &gt; nums2[j]，则 j++。</li>
<li>如果 nums1[i] == nums2[j]，将元素拷贝到 nums1[k]，且 i++，j++，k++。</li>
</ul>
</li>
<li>返回数组 nums1 前 k 个元素。</li>
</ul>
</blockquote>
<h4 id="图示："><a href="#图示：" class="headerlink" title="图示："></a>图示：</h4><p><img src="/2020/05/27/leetcode-1/2.png" alt="排序"></p>
<h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersect</span><span class="params">(self, nums1: [int], nums2: [int])</span> -&gt; [int]:</span></span><br><span class="line">        nums1.sort()</span><br><span class="line">        nums2.sort()</span><br><span class="line">        r = []</span><br><span class="line">        left, right = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; len(nums1) <span class="keyword">and</span> right &lt; len(nums2):</span><br><span class="line">            <span class="keyword">if</span> nums1[left] &lt; nums2[right]:</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> nums1[left] == nums2[right]:</span><br><span class="line">                r.append(nums1[left])</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">                right += <span class="number">1</span>    </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure>



<h3 id="方法三-哈希映射"><a href="#方法三-哈希映射" class="headerlink" title="方法三    哈希映射"></a>方法三    哈希映射</h3><h4 id="算法：-2"><a href="#算法：-2" class="headerlink" title="算法："></a>算法：</h4><blockquote>
<ul>
<li>如果 nums1 元素个数大于 nums2，则交换数组元素。</li>
<li>对于 nums1 的每个元素，添加到 HashMap m 中，如果元素已经存在则增加对应的计数。</li>
<li>初始化 k = 0，记录当前交集元素个数。</li>
<li>遍历数组 nums2：<ul>
<li>检查元素在 m 是否存在，若存在且计数为正：<ul>
<li>将元素拷贝到 nums1[k]，且 k++。</li>
<li>减少 m 中对应元素的计数。</li>
</ul>
</li>
</ul>
</li>
<li>返回 nums1 前 k 个元素。</li>
</ul>
</blockquote>
<h4 id="图示：-1"><a href="#图示：-1" class="headerlink" title="图示："></a>图示：</h4><p><img src="/2020/05/27/leetcode-1/1.png" alt="哈希映射"></p>
<h4 id="程序："><a href="#程序：" class="headerlink" title="程序："></a>程序：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersect</span><span class="params">(self, nums1: [int], nums2: [int])</span> -&gt; [int]:</span></span><br><span class="line">        n1,n2=collections.Counter(nums1),collections.Counter(nums2)</span><br><span class="line">        res=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> n1:</span><br><span class="line">            tmp=min(n1[i],n2[i])</span><br><span class="line">            <span class="keyword">while</span> tmp&gt;<span class="number">0</span>:</span><br><span class="line">                res.append(i)</span><br><span class="line">                tmp-=<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>



<h2 id="进阶问题"><a href="#进阶问题" class="headerlink" title="进阶问题"></a>进阶问题</h2><blockquote>
<p>问题：</p>
<ul>
<li>如果给定的数组已经排好序呢？你将如何优化你的算法？</li>
</ul>
<p>我的答案：</p>
<ul>
<li>不需要优化，如果排好序了这个算法效果更好。</li>
</ul>
</blockquote>
<blockquote>
<p>问题：</p>
<ul>
<li>如果 nums1 的大小比 nums2 小很多，哪种方法更优？</li>
</ul>
<p>我的答案：</p>
<ul>
<li>应该对算法进行优化，先比较一下两个数组的长度，每次循环比较小的数组。</li>
</ul>
</blockquote>
<blockquote>
<p>问题：</p>
<ul>
<li>如果 nums2 的元素存储在磁盘上，磁盘内存是有限的，并且你不能一次加载所有的元素到内存中，你该怎么办？</li>
</ul>
<p>我的答案：</p>
<ul>
<li>分批加载执行</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>算法</tag>
        <tag>简单难度</tag>
        <tag>哈希</tag>
        <tag>递归</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络笔记1 M-P模型</title>
    <url>/2020/05/27/mp/</url>
    <content><![CDATA[<h2 id="神经网络笔记1-–-M-P模型（神经元模型）"><a href="#神经网络笔记1-–-M-P模型（神经元模型）" class="headerlink" title="神经网络笔记1 – M-P模型（神经元模型）"></a>神经网络笔记1 – M-P模型（神经元模型）</h2><p>通过对生物神经元信息处理过程进行了简化和概括。M-P模型的神经元如图。</p>
<p><img src="/2020/05/27/mp/1.png" alt="图1"></p>
<h3 id="神经元的特性包括："><a href="#神经元的特性包括：" class="headerlink" title="神经元的特性包括："></a>神经元的特性包括：</h3><ol>
<li>多输入单输出</li>
<li>不同输入的权值不同</li>
<li>每个神经元都具有阈值</li>
<li>多个输入在处理体中进行累加，超过阈值输出1，小于阈值输出0。具体过程、公式表示如下图。</li>
</ol>
<p><img src="/2020/05/27/mp/2.png" alt="图2"></p>
<p><img src="/2020/05/27/mp/3.png" alt="图3"></p>
<h3 id="激活函数："><a href="#激活函数：" class="headerlink" title="激活函数："></a>激活函数：</h3><p>​        在处理体中判断累加和能否被激活的那个函数就是激活函数。上图中的激活函数就是sign()。具体概念如图。</p>
<p><img src="/2020/05/27/mp/4.png" alt="图4"></p>
<p>常见的激活函数如图：</p>
<p><img src="/2020/05/27/mp/5.png" alt="图5"></p>
]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
